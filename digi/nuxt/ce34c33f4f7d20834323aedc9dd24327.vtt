WEBVTT

00:00:05.960 --> 00:00:07.640
Hello and welcome to the panel

00:00:07.640 --> 00:00:10.040
“The Future of European Digital Policy”

00:00:10.320 --> 00:00:11.680
My name is Klára Votavová

00:00:11.680 --> 00:00:14.120
I’m a member of
the Czech Pirate Party

00:00:14.320 --> 00:00:16.600
and I will guide you through this panel

00:00:17.520 --> 00:00:18.440
Before we start,

00:00:18.520 --> 00:00:22.320
I would like to ask you to participate
in a poll that we just sent you

00:00:22.720 --> 00:00:26.680
We will reflect on your answers
at the end of our discussion 

00:00:27.760 --> 00:00:31.240
This December is an exciting month
for digital regulation

00:00:31.480 --> 00:00:34.680
because it is when the European
Commission unveils

00:00:34.680 --> 00:00:38.560
some highly anticipated pieces
of digital legislation.

00:00:39.280 --> 00:00:41.840
Specifically, on 15th December

00:00:42.080 --> 00:00:44.640
the Commission should unveil a package

00:00:44.640 --> 00:00:46.400
containing two crucial proposals.

00:00:46.560 --> 00:00:49.320
The first of them is, of course,
the Digital Services Act

00:00:49.880 --> 00:00:52.680
which should set responsibilities
of online platforms

00:00:52.680 --> 00:00:54.600
regarding the content
that they carry.

00:00:54.920 --> 00:00:57.720
The second act in this package is
the Digital Markets Act

00:00:57.920 --> 00:01:00.760
which should clarify rules
for very large platforms,

00:01:00.800 --> 00:01:02.360
such as Facebook or Amazon,

00:01:02.400 --> 00:01:05.920
which effectively regulate access
to certain segments of the internet.

00:01:06.480 --> 00:01:08.280
This week, the Commission also unveiled

00:01:08.280 --> 00:01:10.320
the so-called European
Democracy Action Plan

00:01:10.640 --> 00:01:12.720
which among other things
tackles disinformation

00:01:12.720 --> 00:01:14.600
and online political advertising

00:01:14.920 --> 00:01:16.080
And last but not least,

00:01:16.360 --> 00:01:18.800
in March the Commission
is also expected

00:01:18.800 --> 00:01:23.160
to propose a regulation on some
applications of artificial intelligence

00:01:23.960 --> 00:01:24.760
In short,

00:01:24.920 --> 00:01:27.880
we live in exciting times
for digital regulation

00:01:28.120 --> 00:01:31.560
and I am really looking forward
to discussing it with you.

00:01:32.520 --> 00:01:33.720
Without further ado,

00:01:33.720 --> 00:01:36.200
let me introduce our four
honorable panelists.

00:01:36.680 --> 00:01:39.000
The first of them is, of course,
George Tilesch

00:01:39.000 --> 00:01:42.000
that you have maybe
already listened to

00:01:42.000 --> 00:01:45.000
in the dialogue with Aneta Zachová

00:01:45.840 --> 00:01:48.920
George consults on AI governance

00:01:48.920 --> 00:01:51.120
among other places in the Silicon Valley

00:01:51.280 --> 00:01:53.720
And he has also co-authored a book,

00:01:53.920 --> 00:01:56.480
published this year called
“Between Brains”

00:01:56.480 --> 00:01:57.400
Welcome George

00:01:58.400 --> 00:02:02.040
Our second guest is Eliška Pirková,

00:02:02.040 --> 00:02:05.040
a lawyer at the Access Now NGO

00:02:05.840 --> 00:02:08.560
who focuses on freedom of speech
on the internet

00:02:08.600 --> 00:02:10.840
and liability of platforms
for content

00:02:11.440 --> 00:02:12.440
Welcome Eliška

00:02:13.480 --> 00:02:18.480
The third panelist is the vice president
of the European Parliament,

00:02:18.760 --> 00:02:20.600
the Pirate MEP Marcel Kolaja.

00:02:20.960 --> 00:02:25.000
Marcel focuses both on the regulation
of artificial intelligence

00:02:25.200 --> 00:02:28.760
and on platform liability
in the European Parliament.

00:02:29.600 --> 00:02:30.760
And last but not least,

00:02:30.760 --> 00:02:33.000
I’m glad to welcome Pavel Havlíček

00:02:33.000 --> 00:02:36.480
Analyst at the Association for
International Affairs

00:02:36.840 --> 00:02:40.880
who is focused mainly on
online political advertising

00:02:40.920 --> 00:02:42.480
So welcome to all of you

00:02:42.960 --> 00:02:45.880
And now let us jump right
into the first topic

00:02:45.920 --> 00:02:48.480
which is the upcoming
EU digital regulation.

00:02:49.600 --> 00:02:51.840
My first question goes to Marcel

00:02:52.480 --> 00:02:55.680
We know that you have been dealing
with some recent EU legislation

00:02:55.680 --> 00:02:59.320
that is relevant to platform's
liability for content

00:02:59.560 --> 00:03:02.160
including the terrorist
content regulation

00:03:02.320 --> 00:03:05.400
and also passively the copyright
directive of the last year

00:03:05.920 --> 00:03:09.160
What are the key pitfalls of these
specific pieces of legislation?

00:03:09.880 --> 00:03:10.880
Hello everybody

00:03:11.080 --> 00:03:14.080
And I apologise that you
cannot see my video

00:03:15.120 --> 00:03:16.440
I cannot see yours either.

00:03:18.120 --> 00:03:20.400
Now to your question

00:03:23.080 --> 00:03:24.920
I mean, if you have two hours to listen

00:03:24.920 --> 00:03:29.120
then I can go through every single
concern that I have

00:03:30.560 --> 00:03:33.480
but I’ll try to make it short

00:03:34.200 --> 00:03:39.560
and maybe the most
overarching principle

00:03:41.880 --> 00:03:47.000
that we can see both
in the terrorism regulation

00:03:48.120 --> 00:03:50.960
but also in the copyright directive

00:03:50.960 --> 00:03:55.400
and in terms of my concern

00:03:56.520 --> 00:03:58.680
is that

00:03:59.760 --> 00:04:01.520
I sometimes have the feeling

00:04:02.880 --> 00:04:05.560
that the legislation has been created

00:04:05.560 --> 00:04:09.000
only with the largest platforms
in mind

00:04:10.840 --> 00:04:11.840
And

00:04:13.160 --> 00:04:18.680
I also need to say that I followed
the implementation

00:04:19.880 --> 00:04:21.960
of the copyright directive

00:04:21.960 --> 00:04:25.080
which is now in the member states
very closely

00:04:25.840 --> 00:04:28.480
I questioned the Commission on

00:04:29.920 --> 00:04:32.680
on how

00:04:34.120 --> 00:04:38.360
how France is approaching
the legislation

00:04:38.360 --> 00:04:44.320
and it looks like that the path
that France has chosen

00:04:44.560 --> 00:04:49.720
when it comes to protection of users,
and therefore citizens,

00:04:49.760 --> 00:04:53.120
and their fundamental rights
is insufficient

00:04:53.200 --> 00:04:57.120
And I think that these two things
are very much connected

00:04:57.200 --> 00:04:59.000
If you think about the internet

00:04:59.200 --> 00:05:01.120
like it's only basically 
Facebook and YouTube

00:05:01.360 --> 00:05:05.160
it might be easy to, you know,
draft some legislation

00:05:05.280 --> 00:05:07.320
and it might have been easy to think

00:05:07.320 --> 00:05:10.120
"Well, yeah, OK, so we have these platforms here

00:05:10.120 --> 00:05:11.960
where people share content”

00:05:11.960 --> 00:05:15.600
and it would be fairly easy, you know,
to target them

00:05:15.600 --> 00:05:18.720
as kind of like gatekeepers
to information of that

00:05:18.720 --> 00:05:22.080
If there is something illegal,
they can easily remove it

00:05:22.200 --> 00:05:23.640
But the problem is that

00:05:23.720 --> 00:05:28.720
first it creates the environment for
private law enforcement,

00:05:28.720 --> 00:05:32.680
where these large platforms
would decide

00:05:32.960 --> 00:05:36.600
on what is legal and
what is illegal

00:05:36.920 --> 00:05:43.720
And as much as we've been hearing
very often recently that

00:05:44.040 --> 00:05:48.080
what is illegal offline should 
also be illegal online

00:05:48.280 --> 00:05:49.960
I also need to say

00:05:50.040 --> 00:05:52.480
that it’s very important to

00:05:52.480 --> 00:05:55.000
pay attention to
the opposite principle

00:05:55.440 --> 00:06:00.880
that what is legal offline
should also be legal online

00:06:00.880 --> 00:06:02.640
and for that we need protection

00:06:02.640 --> 00:06:06.560
And we  especially need protection
for the freedom of speech

00:06:07.160 --> 00:06:12.520
and if we allow these large companies
to do policing,

00:06:12.600 --> 00:06:17.320
to decide what is legal
and what is not

00:06:17.360 --> 00:06:20.120
then we have basically
no protections

00:06:20.160 --> 00:06:24.080
And if we put them in a place

00:06:24.080 --> 00:06:28.240
where they are even liable for
the content that is shared

00:06:28.240 --> 00:06:29.640
on these platforms

00:06:29.640 --> 00:06:30.800
that is even worse,

00:06:31.200 --> 00:06:35.480
because then they are simply
incentivised to remove

00:06:36.160 --> 00:06:40.680
anything that gets even close,

00:06:41.080 --> 00:06:41.880
just close,

00:06:41.880 --> 00:06:43.720
to maybe be illegal

00:06:44.000 --> 00:06:47.560
because it's much safer for them
to remove such content

00:06:47.920 --> 00:06:51.280
and not to face any kind of sanctions.

00:06:51.480 --> 00:06:53.160
So that is one problem. 

00:06:53.440 --> 00:06:56.320
Another problem is that

00:06:56.360 --> 00:06:58.920
if you look at tetherism regulation,

00:06:58.920 --> 00:07:02.280
the removal of terrorist
content online

00:07:03.000 --> 00:07:06.440
One of the biggest issues
where the Council

00:07:06.440 --> 00:07:08.800
and the Parliament cannot
really agree on

00:07:09.440 --> 00:07:14.200
is how to deal with cross-border
removal orders

00:07:14.240 --> 00:07:15.760
So imagine

00:07:16.120 --> 00:07:16.680
You know

00:07:16.680 --> 00:07:21.400
The purpose of this legislation is
to remove terrorism content

00:07:21.960 --> 00:07:22.920
from the internet

00:07:23.360 --> 00:07:29.880
and now imagine that you get
a removal order

00:07:29.880 --> 00:07:33.600
from some competent authority
from Hungary

00:07:34.160 --> 00:07:38.720
where all of us know that

00:07:40.360 --> 00:07:42.640
we've been having issues
with the rule of law

00:07:44.000 --> 00:07:47.240
and now imagine that

00:07:49.240 --> 00:07:54.680
the government will start using
this legislation for censorship reasons,

00:07:56.440 --> 00:07:59.680
that they will try to
censor opposition

00:07:59.680 --> 00:08:04.920
They could say that the Greens,
for instance,

00:08:04.920 --> 00:08:07.160
are Green terrorists

00:08:07.560 --> 00:08:09.640
and they start removing
their content

00:08:11.560 --> 00:08:14.920
Now that is an issue per se

00:08:15.920 --> 00:08:23.880
But now imagine that they would be able 
to do it across all Europe

00:08:24.440 --> 00:08:29.880
so with the principle of
cross-border removal

00:08:29.880 --> 00:08:33.120
when we have majority of
these big platforms,

00:08:33.120 --> 00:08:35.760
for instance,
in Ireland established

00:08:36.080 --> 00:08:42.120
and the Hungarian authority can request
a company established in Ireland

00:08:42.640 --> 00:08:45.280
to remove content Europe-wide

00:08:45.720 --> 00:08:51.840
then we are basically creating this
race to the bottom principle that

00:08:52.240 --> 00:08:53.680
if you target any-

00:08:54.280 --> 00:08:56.760
if you want to remove some content
and you target

00:08:58.160 --> 00:09:02.320
you target a member state with
the lowest level of

00:09:02.320 --> 00:09:04.880
protection for fundamental rights,
for freedom of speech,

00:09:04.880 --> 00:09:09.680
you can get that content removed
for the whole Europe.

00:09:10.280 --> 00:09:12.960
Now I would like to turn to
Eliška Pirková

00:09:13.080 --> 00:09:16.320
who made an excellent video for us
on the Digital Services Act

00:09:16.320 --> 00:09:17.600
that Marcel also mentioned.

00:09:18.480 --> 00:09:20.200
Can you please shortly tell us

00:09:20.200 --> 00:09:23.240
why the Digital Services Act
is so important

00:09:23.400 --> 00:09:27.040
and what are your key recommendations
for how it should look like?

00:09:27.360 --> 00:09:29.480
Thank you very much
and hello everyone

00:09:29.480 --> 00:09:31.400
it’s a great pleasure to be here.

00:09:31.400 --> 00:09:33.880
Just to quickly refresh
the memory of everyone,

00:09:33.880 --> 00:09:34.800
my name is Eliška,

00:09:34.800 --> 00:09:36.520
I work as a Europe Policy Analyst

00:09:36.520 --> 00:09:38.280
at Access Now
in our Brussels team

00:09:38.760 --> 00:09:41.560
and I lead our work on content
governance issues

00:09:41.560 --> 00:09:43.760
and specifically on
Digital Services Act

00:09:43.760 --> 00:09:46.120
and partially touching
on Digital Markets Act,

00:09:46.120 --> 00:09:49.080
which is the second law which
will be coming out of this

00:09:49.080 --> 00:09:50.320
big legislative package.

00:09:50.640 --> 00:09:52.960
And I think that this question
nicely bridges

00:09:52.960 --> 00:09:57.080
to those points that actually Marcel
raised during his contribution

00:09:58.200 --> 00:10:00.520
so why DSA is important

00:10:01.360 --> 00:10:04.040
it’s precisely due to those
regulatory trends

00:10:04.040 --> 00:10:06.640
that we were witnessing in Europe
for the last five years

00:10:06.640 --> 00:10:07.800
or even longer

00:10:08.080 --> 00:10:12.160
whether at EU level
or national level 

00:10:12.240 --> 00:10:17.440
But we often encounter rather hastily
drafted regulatory responses

00:10:17.440 --> 00:10:21.440
to certain categories of illegal
or potentially harmful content,

00:10:21.920 --> 00:10:25.520
ultimately shifts state obligations
and responsibilities

00:10:25.520 --> 00:10:28.120
from states on private actors

00:10:28.360 --> 00:10:30.720
Private actors then
are responsible for

00:10:30.720 --> 00:10:32.640
balancing [inaudible]
and financial rights,

00:10:32.640 --> 00:10:35.120
they are hardly subjected
to any public scrutiny,

00:10:35.120 --> 00:10:38.200
transparency and
judicial oversight,

00:10:38.200 --> 00:10:39.880
which is another major issue

00:10:39.880 --> 00:10:42.240
within the online terrorist
content regulation,

00:10:42.400 --> 00:10:43.720
is lacking behind.

00:10:44.320 --> 00:10:46.360
This is then directly connected
to the fact that

00:10:46.360 --> 00:10:49.440
users usually don’t have easy access
to effective remedy,

00:10:49.520 --> 00:10:52.400
appealment mechanism
or other channels and options

00:10:52.400 --> 00:10:54.280
to actually challenge
those decisions.

00:10:55.160 --> 00:10:56.320
On one hand in the past,

00:10:56.320 --> 00:10:58.320
the EU tried to sort it out through

00:10:58.320 --> 00:11:00.480
creating different
co-regulatory measures,

00:11:00.480 --> 00:11:03.000
we saw Code of Conduct
and Code of Practices,

00:11:03.320 --> 00:11:06.880
but again they often served
as pushing companies

00:11:06.880 --> 00:11:09.440
to the corner and remove
the content faster

00:11:09.800 --> 00:11:11.800
And trying to make them
comply this way,

00:11:11.800 --> 00:11:14.720
using legally binding regulations
as sort of a tract

00:11:14.720 --> 00:11:17.040
If you do not fulfill
these requirements,

00:11:17.760 --> 00:11:19.120
then we will regulate you.

00:11:19.920 --> 00:11:21.560
Now DSA is important

00:11:21.560 --> 00:11:24.280
because nothing needs to be
probably clearly set,

00:11:24.760 --> 00:11:30.200
and we need systemic regulation
of online gatekeepers

00:11:30.200 --> 00:11:31.200
or large platforms

00:11:31.200 --> 00:11:34.560
And we finally need to make the
regulation working in the right way

00:11:35.680 --> 00:11:37.920
We need to careful
when crafting DSA

00:11:37.920 --> 00:11:40.400
that we won't reinforce the already
existing dominance

00:11:40.400 --> 00:11:41.880
of online gatekeepers

00:11:41.880 --> 00:11:45.120
and we need to acknowledge that there
are smaller actors and players

00:11:45.120 --> 00:11:48.200
who can be negatively impacted by
such regulatory measures

00:11:48.880 --> 00:11:50.400
But the systemic regulation

00:11:50.400 --> 00:11:53.840
that will actually harmonise the
regulatory landscape

00:11:53.840 --> 00:11:54.800
across the EU

00:11:54.800 --> 00:11:57.360
is absolutely necessary
at this point.

00:11:58.440 --> 00:12:01.760
DSA as the law from this package

00:12:01.760 --> 00:12:03.760
will specifically look at 
the issues of

00:12:03.760 --> 00:12:05.440
something we call
content governance

00:12:05.440 --> 00:12:07.280
so how the illegal content
is actually

00:12:07.280 --> 00:12:09.440
going to be regulated in the EU,

00:12:10.040 --> 00:12:12.800
It will hopefully preserve
the liability model

00:12:12.800 --> 00:12:15.520
as it currently exists under
the E-Commerce directives,

00:12:15.520 --> 00:12:17.880
so under the article 14 of
the E-commerce directive,

00:12:18.320 --> 00:12:21.640
which establishes the exact reels for
conditional models of liability

00:12:22.200 --> 00:12:24.920
There are other principles within
the current E-commerce directive

00:12:24.920 --> 00:12:26.720
that should be upheld by DSA,

00:12:26.720 --> 00:12:28.800
such as promotion of
general monitoring,

00:12:28.800 --> 00:12:30.480
and I don’t want to go into details

00:12:30.680 --> 00:12:32.440
and discuss all these
legal principles

00:12:32.440 --> 00:12:34.520
but I’m happy to answer
all the questions

00:12:34.520 --> 00:12:35.880
that will follow afterwards.

00:12:36.400 --> 00:12:40.240
But what we specifically hopefully
establish is that

00:12:40.240 --> 00:12:42.120
finally users fundamental rights,

00:12:42.120 --> 00:12:43.320
users' empowerment

00:12:43.320 --> 00:12:45.800
and users' control
over their personal data

00:12:45.800 --> 00:12:48.680
and over information they seek
and they impart

00:12:49.120 --> 00:12:51.160
will actually return back
to the user

00:12:51.360 --> 00:12:54.000
it will enable proper
public oversight

00:12:54.040 --> 00:12:57.040
where actually authorities
can really follow

00:12:57.080 --> 00:13:01.440
whether companies do comply with
the new procedures required

00:13:01.440 --> 00:13:02.440
and requirements

00:13:02.680 --> 00:13:04.160
that these laws will establish,

00:13:04.200 --> 00:13:06.560
given that it will be
clearly determined

00:13:06.800 --> 00:13:08.320
what the states are supposed to do

00:13:08.320 --> 00:13:10.400
and what is the role of
private platforms

00:13:10.640 --> 00:13:12.720
So we will finally reinforce
something we call

00:13:12.720 --> 00:13:14.600
the principle of legal certainty

00:13:14.600 --> 00:13:18.200
which is the main precondition
and integral part of law

00:13:19.040 --> 00:13:23.400
and finally it will also hopefully
enable something we call

00:13:23.400 --> 00:13:25.880
the Research Data Access Framework

00:13:26.000 --> 00:13:28.640
So we often advocate
for research-based

00:13:28.640 --> 00:13:30.520
and evidence-based policy making

00:13:30.520 --> 00:13:33.560
so we can actually avoid haste
and draft regulations

00:13:33.560 --> 00:13:35.160
that we've assessed to this day,

00:13:35.440 --> 00:13:37.280
that put the emphasis on a quick 

00:13:37.280 --> 00:13:39.280
and fast removal of the content

00:13:40.080 --> 00:13:41.560
without properly understanding

00:13:41.560 --> 00:13:43.520
what platforms do
with that content,

00:13:43.520 --> 00:13:44.520
how they moderate,

00:13:45.360 --> 00:13:48.520
whether automated decision-making
processes are being involved,

00:13:48.520 --> 00:13:50.000
we know that they are obviously,

00:13:50.000 --> 00:13:52.960
but what decisions were done
purely by the automation

00:13:52.960 --> 00:13:55.000
and when the human in the loop
is being kept

00:13:55.000 --> 00:13:58.280
that's not clearly stated
elsewhere

00:13:58.600 --> 00:14:01.320
And it will also reinforce
something we call

00:14:01.320 --> 00:14:02.640
meaningful transparency,

00:14:02.840 --> 00:14:04.600
that will enable all of this.

00:14:04.840 --> 00:14:07.400
Right now transparency reports
are more,

00:14:07.760 --> 00:14:10.160
you know, it’s a voluntary commitment
of platforms

00:14:10.160 --> 00:14:11.080
to publish those

00:14:11.080 --> 00:14:14.640
but I think it’s time we cannot
simply treat transparency

00:14:14.640 --> 00:14:17.040
as some form of generosity
or luxury

00:14:17.040 --> 00:14:19.320
that the platforms
occasionally express.

00:14:19.480 --> 00:14:22.720
They should be actually mandatory
obligations for platforms to do so

00:14:23.080 --> 00:14:24.000
due to today dominance

00:14:24.000 --> 00:14:24.760
and the impact 

00:14:24.760 --> 00:14:27.080
on the public and democratic
discourse they have 

00:14:27.480 --> 00:14:30.800
DSA hopefully together with
DMA will actually do

00:14:30.800 --> 00:14:32.760
much more than this
that I've just stated 

00:14:32.760 --> 00:14:34.120
There is a whole question

00:14:34.120 --> 00:14:36.600
about what kind of oversight
we actually will have

00:14:36.600 --> 00:14:38.360
and the DSA cut to the power.

00:14:39.520 --> 00:14:42.560
It will probably also harmonise
the response to the

00:14:42.880 --> 00:14:45.720
how we actually currently
regulate illegal content

00:14:45.720 --> 00:14:47.920
through harmonisation of
something we call

00:14:47.920 --> 00:14:49.640
Notice and Action Procedures,

00:14:49.640 --> 00:14:53.600
so that these adequate response
mechanisms are perhaps tailored

00:14:53.600 --> 00:14:56.320
to the particular type of content
they’re supposed to tackle

00:14:56.760 --> 00:15:00.000
and that they enable and empower
users to report that content

00:15:00.000 --> 00:15:02.640
so they will actually be
easily accessible

00:15:03.200 --> 00:15:06.440
and they won’t nudge users away
from even, you know,

00:15:07.280 --> 00:15:10.080
taking care of what kind of platform
and online environment

00:15:10.080 --> 00:15:12.040
they coexist with
other communities.

00:15:12.200 --> 00:15:13.200
Thank you Eliška

00:15:13.720 --> 00:15:17.320
Maybe I would quickly
jump back to Marcel,

00:15:17.400 --> 00:15:20.480
if you would like to react on
Eliška’s comments on the DSA

00:15:20.600 --> 00:15:21.600
But quickly

00:15:22.080 --> 00:15:22.640
Yeah, thanks

00:15:23.520 --> 00:15:27.800
I don’t necessarily need to,
you know, react

00:15:27.840 --> 00:15:32.600
but what I would like to
underline is that

00:15:33.960 --> 00:15:34.480
that

00:15:35.840 --> 00:15:38.960
there are these three very
important principles

00:15:39.720 --> 00:15:41.280
of the E-commerce directive

00:15:41.280 --> 00:15:45.120
and we should make sure
that we do not change them

00:15:45.600 --> 00:15:46.640
in the new legislation

00:15:46.640 --> 00:15:49.080
which is also what the
Commission promised to do

00:15:49.480 --> 00:15:53.960
and the Digital Services Act is
going to be publicly published,

00:15:53.960 --> 00:15:55.560
according to the Commission,

00:15:55.760 --> 00:15:57.920
on the 15th December

00:15:58.120 --> 00:16:02.560
so I’m very much looking forward to see
what the Commission has drafted,

00:16:02.840 --> 00:16:06.840
but these 3 principles of No General
Monitoring Obligation,

00:16:07.480 --> 00:16:12.600
of the Liability Exception

00:16:12.840 --> 00:16:14.640
and of the Country of Origin

00:16:14.720 --> 00:16:19.400
is definitely that we need to
make sure that are there

00:16:19.800 --> 00:16:22.640
and then we need to

00:16:23.880 --> 00:16:28.800
we need to have protections
for fundamental rights online,

00:16:30.360 --> 00:16:34.440
we need to have a
Notice and Action System

00:16:34.480 --> 00:16:42.760
that will enable removals
of illegal content

00:16:43.160 --> 00:16:47.360
but also has exactly these protections
of fundamental rights

00:16:47.760 --> 00:16:50.400
And in the Digital Markets Act,

00:16:50.400 --> 00:16:53.080
which is part of the package
of the Digital Services Act,

00:16:54.600 --> 00:16:59.320
we need to make sure that
we foster innovation

00:16:59.800 --> 00:17:02.960
by making services interoperable

00:17:02.960 --> 00:17:07.240
so that, you know, large platforms,
like Facebook for instance,

00:17:07.440 --> 00:17:11.160
would have the obligation
to enable

00:17:11.440 --> 00:17:15.400
interconnectivity with
smaller platforms,

00:17:15.440 --> 00:17:19.280
with community-based free software
like MastaDone for instance,

00:17:19.440 --> 00:17:21.680
the chat systems could be
interoperable

00:17:21.680 --> 00:17:28.160
so that users can connect to
users of another platform

00:17:28.320 --> 00:17:32.440
while not being in the need of having
seven different accounts.

00:17:32.960 --> 00:17:35.760
Thank you very much for
these remarks Marcel

00:17:36.880 --> 00:17:40.640
We may get back to the
Digital Services Act again

00:17:40.640 --> 00:17:44.480
but now I would like to turn to
another piece of legislation

00:17:45.120 --> 00:17:47.800
The European Democracy Action Plan

00:17:47.840 --> 00:17:50.800
which was actually released
only the past week

00:17:51.760 --> 00:17:55.240
and which deals with disinformation

00:17:55.240 --> 00:17:57.680
and also political advertising

00:17:57.840 --> 00:17:59.960
And here I would like to ask
Pavel Havlíček

00:18:00.440 --> 00:18:01.800
if he would like to chip in

00:18:01.800 --> 00:18:05.640
and maybe tell us what are
his initial thoughts

00:18:05.640 --> 00:18:07.640
on this piece of legislation

00:18:07.640 --> 00:18:10.000
or actually it’s a non-regulatory piece

00:18:10.000 --> 00:18:11.920
but nevertheless,
action plan

00:18:13.200 --> 00:18:14.200
Thank you so much Klára

00:18:14.520 --> 00:18:16.120
Thank you so much for having me

00:18:17.080 --> 00:18:18.600
My name is Pavel Havlíček

00:18:18.600 --> 00:18:21.040
and I work for the Association
for International Affairs

00:18:21.240 --> 00:18:24.040
and if you are fans as well

00:18:24.040 --> 00:18:26.240
You also saw my video on the topic

00:18:27.040 --> 00:18:30.080
of EU’s Action Plan for Democracy

00:18:30.080 --> 00:18:31.840
which was just presented
on Thursday.

00:18:32.040 --> 00:18:34.320
I believe that this is
the right step forward,

00:18:34.320 --> 00:18:37.400
I will try to explain why
this is innovative

00:18:37.400 --> 00:18:39.120
and brings some new elements and

00:18:40.240 --> 00:18:43.840
you know, especially in the domains
that you already hinted on

00:18:44.320 --> 00:18:45.080
yourself

00:18:45.360 --> 00:18:46.560
Digital political advertising

00:18:46.560 --> 00:18:49.200
but also other issues related
to disinformation

00:18:49.360 --> 00:18:52.960
So if we start maybe
with the broader picture

00:18:52.960 --> 00:18:55.160
then I try to also
describing the video 

00:18:55.320 --> 00:18:55.840
You know,

00:18:56.160 --> 00:18:57.320
there’s the law,
you know,

00:18:57.480 --> 00:18:59.280
in the Democracy Action Plan,
you know,

00:18:59.280 --> 00:19:01.240
I recommend you to have a look
because

00:19:01.480 --> 00:19:03.240
you have quite a
lengthy description

00:19:03.240 --> 00:19:05.880
on what the EU is really
planning to do

00:19:05.880 --> 00:19:08.480
in terms of three major pillars

00:19:08.480 --> 00:19:10.200
of the actual plan

00:19:10.880 --> 00:19:12.480
which are elections,

00:19:12.680 --> 00:19:15.600
these are the fight disinformation

00:19:15.600 --> 00:19:19.560
and finally also
issues of media,

00:19:19.560 --> 00:19:20.800
media plurality

00:19:20.840 --> 00:19:24.400
and also basically,
you know, use,

00:19:24.400 --> 00:19:25.640
interaction with media,

00:19:25.720 --> 00:19:27.800
support the media
and so on and so forth

00:19:27.800 --> 00:19:30.400
So these are the three bigger
categories, you know

00:19:30.680 --> 00:19:35.280
If we have a look especially
at the disinformation [inaudible],

00:19:35.480 --> 00:19:37.920
I understood that this section
should also be about

00:19:38.120 --> 00:19:40.520
putting into wide context
what we are having now,

00:19:40.520 --> 00:19:43.960
which is not only the second biggest
worldwide pandemic

00:19:43.960 --> 00:19:45.160
but also infodemics

00:19:45.200 --> 00:19:49.360
and we are also having huge waves
of disinformation campaigns

00:19:49.920 --> 00:19:52.480
and this is something that
the action plan

00:19:52.480 --> 00:19:54.240
strategic the vision of the EU,

00:19:54.440 --> 00:19:57.040
of the European Commission
to tackle disinformation

00:19:57.040 --> 00:20:01.160
is also basically
responding to it.

00:20:01.160 --> 00:20:05.960
It is going along the lines of
three different levels.

00:20:06.080 --> 00:20:11.160
First is the international or
foreign interference level,

00:20:11.160 --> 00:20:15.840
which is basically using
quite strong language

00:20:15.920 --> 00:20:19.720
on basically naming and shaming
the third parties

00:20:19.720 --> 00:20:20.920
especially Russia and China

00:20:20.920 --> 00:20:25.240
that have engaged in massive
disinformation operations,

00:20:25.240 --> 00:20:27.480
as we know from the past

00:20:27.680 --> 00:20:31.400
and elsewhere from Europe
and the world.

00:20:31.480 --> 00:20:31.920
So

00:20:32.800 --> 00:20:35.200
So this is the first level,
you know,

00:20:35.240 --> 00:20:38.000
and here the Action Plan
actually comes up with

00:20:38.120 --> 00:20:40.680
not only strong language and
singling out these [inaudible]

00:20:40.680 --> 00:20:44.280
but also potentially
introducing sanctions,

00:20:44.480 --> 00:20:45.480
a sanction mechanism

00:20:46.080 --> 00:20:50.040
to really increase the cost of
engaging in similar operations.

00:20:50.040 --> 00:20:52.280
So this is quite significant,

00:20:52.280 --> 00:20:53.040
this is something new

00:20:53.480 --> 00:20:57.520
that Czech EU Commissioner
Věra Jourová came up with

00:20:58.560 --> 00:21:01.640
She has been quite consistent on
this foreign interference

00:21:01.640 --> 00:21:04.120
and very vocal about refusing this,

00:21:04.120 --> 00:21:05.320
so the first level.

00:21:05.600 --> 00:21:09.120
Then if we speak about the
so-called like domestic

00:21:09.120 --> 00:21:13.400
that the European, Czech,
Belgium, and elsewhere

00:21:13.520 --> 00:21:16.360
domestic level of the
disinformation debate

00:21:16.360 --> 00:21:19.880
Here you have already spoken
about the issue of

00:21:19.880 --> 00:21:21.120
digital political advertising

00:21:21.160 --> 00:21:25.720
This is obviously going also
in the direction of elections,

00:21:26.040 --> 00:21:29.880
but it also trying to tackle
the domestic manipulation

00:21:30.160 --> 00:21:31.880
from either political parties

00:21:31.920 --> 00:21:36.000
or also third parties engaged
in political campaigns,

00:21:36.120 --> 00:21:39.240
in political kind of debates

00:21:39.240 --> 00:21:40.920
and public discourse in general,
you know,

00:21:41.200 --> 00:21:44.160
and here the Commission is
proposing to move on

00:21:44.520 --> 00:21:46.680
from the current model
of self-regulation

00:21:46.800 --> 00:21:48.920
that Eliška already hinted on,

00:21:49.200 --> 00:21:53.440
by which the Commission initially
tries to persecute,

00:21:53.440 --> 00:21:54.680
from my prospective,

00:21:54.680 --> 00:21:58.160
own sources of its own responsibility
to the big platforms 

00:21:59.000 --> 00:22:02.080
This is not a sustainable model
as we have seen, you know,

00:22:02.080 --> 00:22:06.120
and there were numerous
basically reports

00:22:06.960 --> 00:22:09.040
showing why this has failed

00:22:09.320 --> 00:22:11.520
including what we have produced

00:22:11.520 --> 00:22:14.280
on the level of member
states in Czechia

00:22:14.960 --> 00:22:16.000
with our team

00:22:16.120 --> 00:22:18.560
but also with colleagues from
The Netherlands and Italian,

00:22:18.560 --> 00:22:22.920
we have shown how this
self-regulation is actually

00:22:24.360 --> 00:22:26.160
you know,
one big failure

00:22:26.160 --> 00:22:29.040
you know, from the side of
what we need to see,

00:22:29.720 --> 00:22:33.000
from the side of
the citizens, users,

00:22:33.000 --> 00:22:35.200
but also from the side of
the European Commission

00:22:35.400 --> 00:22:37.560
because if we have a look
at the five pillars

00:22:37.560 --> 00:22:40.040
of the self-regulatory
Code of Practice

00:22:40.040 --> 00:22:41.320
against disinformation, you know,

00:22:41.600 --> 00:22:44.600
we were looking particularly
in three of them, you know,

00:22:44.600 --> 00:22:46.720
Users and government, you know,
sharing the data

00:22:46.720 --> 00:22:48.560
and Eliška spoke about it briefly,

00:22:48.640 --> 00:22:50.600
nevertheless, so the ads libraries

00:22:50.600 --> 00:22:51.520
we have seen

00:22:51.520 --> 00:22:55.960
on all three levels basically
insufficient, you know,

00:22:57.120 --> 00:22:58.920
huge variety in the platforms,
you know,

00:22:58.920 --> 00:23:01.680
some taking it rather seriously,

00:23:01.680 --> 00:23:02.840
some not at all

00:23:02.840 --> 00:23:06.560
so really huge huge differences
between the platforms

00:23:06.560 --> 00:23:08.520
and also on different levels
as well

00:23:08.520 --> 00:23:08.960
So

00:23:09.320 --> 00:23:11.520
So here the Commission is
proposing to move on

00:23:11.680 --> 00:23:13.360
from the principle of
self-regulation

00:23:13.360 --> 00:23:16.200
into more of a co-regulatory approach

00:23:16.480 --> 00:23:18.600
in which the Commission
and the EU in general

00:23:18.600 --> 00:23:20.000
will be turning the table,
you know,

00:23:20.120 --> 00:23:23.000
and really positioning itself
in the middle of the debate

00:23:23.000 --> 00:23:25.280
and really saying quite clearly,
you know,

00:23:25.280 --> 00:23:26.800
what we need from the platforms

00:23:27.040 --> 00:23:31.600
in terms of, for example,
information about microtargeting

00:23:31.600 --> 00:23:32.440
which is quite

00:23:32.640 --> 00:23:33.920
the Action Plan is heavy on this

00:23:34.600 --> 00:23:37.680
Again, on the digital political
advertising you know,

00:23:37.800 --> 00:23:40.400
how we need to be informed,
you know,

00:23:40.520 --> 00:23:42.080
how the report should be looking,

00:23:42.080 --> 00:23:44.440
what kind of set of data
there should be

00:23:44.440 --> 00:23:45.440
and so and so forth.

00:23:45.520 --> 00:23:48.240
So this is again quite locant,
you know,

00:23:48.240 --> 00:23:51.360
and the right step
in the right direction.

00:23:51.400 --> 00:23:53.600
So this is important as well

00:23:54.360 --> 00:23:55.160
and also,

00:23:55.360 --> 00:23:59.320
and here I would jump to the third
kind of level of the whole debate

00:23:59.560 --> 00:24:02.960
This is the very
significant recognition

00:24:02.960 --> 00:24:05.240
of the added value from
the civil society,

00:24:05.400 --> 00:24:06.280
from academia

00:24:06.280 --> 00:24:09.640
but also from journalists
and the media community

00:24:09.640 --> 00:24:11.760
that the Commission itself
cannot do it alone

00:24:12.280 --> 00:24:14.560
Obviously there are also
member states

00:24:14.560 --> 00:24:17.440
that have a very important
role to play

00:24:17.720 --> 00:24:19.600
 in this debate, you know,

00:24:19.600 --> 00:24:22.280
especially when we speak about
elections and electoral processes,

00:24:22.560 --> 00:24:25.520
you can be only self-regulating
the EU elections

00:24:25.520 --> 00:24:27.960
which is, you know, in
the European Parliament indeed

00:24:28.240 --> 00:24:31.720
but obviously we need to do much more
also on the national level

00:24:31.720 --> 00:24:35.080
and here the Commission is
once again pushing for

00:24:35.320 --> 00:24:36.800
multi state ward approach

00:24:36.800 --> 00:24:38.320
and really doing much
more together,

00:24:38.480 --> 00:24:40.560
so that is why I believe

00:24:40.560 --> 00:24:42.680
that we are going
in the right direction

00:24:43.200 --> 00:24:46.760
I still see that there are
some weak spots,

00:24:46.760 --> 00:24:49.640
you know, weak blind spots
in the debate, you know

00:24:50.280 --> 00:24:53.440
We have seen quite little to be,
you know,

00:24:53.960 --> 00:24:56.240
The language on digital advertising,
for example,

00:24:56.240 --> 00:24:58.600
is in general going
in the right direction,

00:24:58.880 --> 00:25:03.160
Europe also recognised in the
freedom of speech, you know,

00:25:03.160 --> 00:25:05.920
and also obviously
digital transparency

00:25:06.360 --> 00:25:08.120
these are the two big
approaches that

00:25:08.120 --> 00:25:10.080
the Commission has decided to take

00:25:10.360 --> 00:25:11.240
This is very good

00:25:11.240 --> 00:25:14.120
but we still see rather
vague language, you know,

00:25:14.240 --> 00:25:17.240
we are so much waiting for
the Digital Services Act

00:25:17.320 --> 00:25:19.720
that needs to be interacting
with each other

00:25:19.720 --> 00:25:22.320
So this will be significant,
you know.

00:25:22.560 --> 00:25:24.560
We will see more in two weeks time

00:25:24.720 --> 00:25:26.240
as was mentioned as well

00:25:26.600 --> 00:25:30.000
So the Commission and the Action
Plan has already hinted on

00:25:30.400 --> 00:25:33.160
on the division of levels a little bit,
you know,

00:25:33.160 --> 00:25:36.320
there is a little bit of, you know,
in the text that

00:25:36.840 --> 00:25:39.120
that DSA should be more
horizontal you know,

00:25:39.120 --> 00:25:41.200
it should filtering out
illegal content

00:25:41.200 --> 00:25:44.000
but also stating the
rules of the game

00:25:44.360 --> 00:25:46.360
to secure the level
playing field indeed

00:25:46.480 --> 00:25:51.680
and the Action Plan for Democracy
should be especially going after

00:25:51.680 --> 00:25:53.120
the harmful content,
you know,

00:25:53.240 --> 00:25:55.280
such as the area
of disinformation

00:25:55.280 --> 00:25:56.760
that I’ve already spoken about

00:25:57.000 --> 00:25:59.040
but still this is to be seen,
you know,

00:25:59.040 --> 00:26:01.360
and we need to be very vocal that

00:26:01.760 --> 00:26:02.760
you know,
about

00:26:03.360 --> 00:26:05.400
that  there should be a clear
division of labour

00:26:05.400 --> 00:26:09.720
and none of the chosen sides should be
really forgettable 

00:26:09.960 --> 00:26:12.640
One last thing that I’ve already said

00:26:12.800 --> 00:26:16.280
during the video recording yesterday
that I was preparing

00:26:16.440 --> 00:26:21.040
was that one last weak spot
of the Action Plan is

00:26:21.720 --> 00:26:23.920
the wider issue of civil society
in the EU,

00:26:23.920 --> 00:26:26.080
this is going beyond the digital space
in the EU

00:26:26.080 --> 00:26:28.160
this is also about part
of the debate,

00:26:28.440 --> 00:26:31.360
because we need both
the recognition that

00:26:31.680 --> 00:26:35.000
the same is for journalists
and media,

00:26:35.160 --> 00:26:36.160
which are pressured

00:26:36.160 --> 00:26:39.240
and then their information
has done an excellent job

00:26:39.280 --> 00:26:42.720
to recognise the fret, you know,
and the problem

00:26:42.720 --> 00:26:46.200
but the same is actually
being faced by the society,

00:26:46.200 --> 00:26:49.560
there is middle on the civic space
for civil society

00:26:49.560 --> 00:26:53.640
or on how the Commission will be
interacting with the civil society

00:26:54.080 --> 00:26:55.000
offline,
you know

00:26:55.000 --> 00:26:56.560
in the wide sense,
you know,

00:26:57.240 --> 00:26:58.280
in the future.

00:26:58.480 --> 00:27:01.240
So happy to elaborate on some of
these points, you know,

00:27:01.240 --> 00:27:03.760
these are some of my brief remarks,
you know,

00:27:04.000 --> 00:27:06.200
on the Action Plan for Democracy

00:27:06.200 --> 00:27:08.400
there is indeed much more
to be discussed, you know,

00:27:08.400 --> 00:27:12.120
I’ve barely touched upon
the elections sphere

00:27:12.120 --> 00:27:14.880
or the area of media

00:27:14.880 --> 00:27:17.840
but there is a really substantial
part on that as well

00:27:17.840 --> 00:27:20.720
so I highly recommend to
have a look at this,

00:27:21.120 --> 00:27:22.440
it’s interesting reading,

00:27:22.440 --> 00:27:24.400
in many respects really innovative

00:27:24.400 --> 00:27:26.080
and going into the right direction

00:27:26.240 --> 00:27:27.120
Thank you

00:27:27.760 --> 00:27:30.600
Thank you Pavel for your description

00:27:30.600 --> 00:27:34.560
or the remarks of the European
Democracy Action Plan

00:27:35.040 --> 00:27:39.120
and finally to cover all crucial
pieces of this legislation

00:27:39.120 --> 00:27:40.880
I would also like to turn to George.

00:27:41.240 --> 00:27:44.720
I know that you are maybe not
an insider that much

00:27:44.720 --> 00:27:47.520
on like specific pieces of
European legislation.

00:27:47.760 --> 00:27:48.760
On the other hand,

00:27:49.880 --> 00:27:53.240
I would like to ask you if there are
like some core principles

00:27:53.240 --> 00:27:58.920
that you think that the EU should put
into its horizontal AI regulation

00:27:58.920 --> 00:28:01.840
that is upcoming in March?

00:28:01.840 --> 00:28:06.720
Like maybe two or three principles
for good AI regulation, 

00:28:06.720 --> 00:28:07.600
if you will.

00:28:08.400 --> 00:28:10.880
So I’m working with a number of
governments worldwide

00:28:10.880 --> 00:28:14.840
and I can see some level of
cross-pollination already happening

00:28:15.840 --> 00:28:18.880
in some of the aspects and
some of the European values

00:28:18.880 --> 00:28:23.080
that they are actually permeating this
kinds of new pieces of legislation

00:28:23.400 --> 00:28:26.560
I’m kind of familiar with
the latest proposal

00:28:26.560 --> 00:28:28.240
that came out from the Parliament

00:28:28.240 --> 00:28:31.600
that is trying to address risk,

00:28:31.840 --> 00:28:35.360
which is probably the most
fundamental part of-

00:28:35.480 --> 00:28:39.320
I see it as a good approach
to approaching AI

00:28:39.600 --> 00:28:44.560
and I’m seeing the US Federal Government
going in a similar direction

00:28:45.840 --> 00:28:49.880
without going broad enough,
I would say.

00:28:50.000 --> 00:28:53.880
So the initiatives that I’m part of
in the US

00:28:53.880 --> 00:28:57.600
are mostly focusing on the government
uses of these technologies

00:28:57.680 --> 00:29:01.440
but finally the ethical and
risk-based principles

00:29:01.440 --> 00:29:02.720
are becoming central

00:29:03.120 --> 00:29:07.120
to what kind of technologies they will
acquire for government purposes

00:29:07.160 --> 00:29:12.960
and what are the technologies
that they can develop in house

00:29:13.160 --> 00:29:17.000
which is, by the way, the majority of
US Federal Government use cases

00:29:18.320 --> 00:29:20.080
What I’m also seeing is that
I'm, you know,

00:29:20.080 --> 00:29:22.480
my headquarters are in California

00:29:22.760 --> 00:29:26.880
and I’m seeing a very direct effect
on European legislation,

00:29:26.880 --> 00:29:29.440
and also partially UK legislation.

00:29:30.240 --> 00:29:31.800
In matters of privacy

00:29:32.240 --> 00:29:34.720
and the general approach to data,

00:29:35.800 --> 00:29:38.280
I’m seeing a number of proposals that

00:29:38.520 --> 00:29:42.200
are kind of being blocked
in the Senate

00:29:42.200 --> 00:29:44.960
because of the standoff
we had during the previous,

00:29:44.960 --> 00:29:46.480
I mean the current administration

00:29:46.480 --> 00:29:48.080
and not the upcoming administration

00:29:49.040 --> 00:29:50.360
in the US that

00:29:50.720 --> 00:29:54.320
that are very much influenced
by a European way of thinking

00:29:54.320 --> 00:29:56.200
especially political advertising

00:29:56.200 --> 00:29:58.760
so just the pieces you have
been talking about

00:29:59.160 --> 00:30:01.120
and fighting disinformation.

00:30:02.160 --> 00:30:03.640
I happen to be,

00:30:04.040 --> 00:30:05.160
I’m a double citizen

00:30:05.160 --> 00:30:07.240
and it gives me the luxury of

00:30:07.240 --> 00:30:09.840
being European-minded
in certain things

00:30:09.840 --> 00:30:11.440
and US-minded in other things

00:30:11.440 --> 00:30:14.560
but I very much welcome your focus

00:30:14.560 --> 00:30:18.840
that I’ve experienced so far in
this session on disinformation.

00:30:19.160 --> 00:30:21.920
I’m very European in that regard.

00:30:22.360 --> 00:30:25.880
I think that disinformation is
probably the biggest issue

00:30:25.880 --> 00:30:28.320
that we have currently
in the realm of AI

00:30:28.320 --> 00:30:31.480
and it attacks the social fabric
very very directly,

00:30:32.600 --> 00:30:36.560
so I see it as the biggest danger
of our times.

00:30:37.320 --> 00:30:41.240
And sometimes I’m as much of
a headliner

00:30:41.400 --> 00:30:45.240
that I’m saying that disinformation
should be regulated similarly

00:30:45.520 --> 00:30:47.080
as nuclear technology is,

00:30:47.440 --> 00:30:51.120
so that may sound
a little bit radical.


00:30:52.360 --> 00:30:55.680
So I think that the principles
and the approaches

00:30:55.680 --> 00:30:58.440
that I’m seeing in upcoming
European legislations

00:30:58.480 --> 00:31:01.640
in this general Artificial
Intelligence framework

00:31:02.160 --> 00:31:03.160
are good ones.

00:31:03.160 --> 00:31:04.520
Of course the proof is
in the pudding,

00:31:05.960 --> 00:31:08.720
as so often with anything
that comes out

00:31:08.720 --> 00:31:10.760
from the Commission
or the Parliament

00:31:12.160 --> 00:31:15.440
because I see that the
risk-based approach

00:31:15.440 --> 00:31:17.640
and establishing a
liability framework

00:31:17.640 --> 00:31:22.040
is something that has been very much
missing from the equation so far.

00:31:22.480 --> 00:31:23.360
We have spent

00:31:23.360 --> 00:31:25.680
and I think Europe has spent
a lot of time,

00:31:25.680 --> 00:31:28.680
probably I would even say
too much time,

00:31:28.960 --> 00:31:29.960
on principles

00:31:30.480 --> 00:31:34.000
so when you’re actually thinking me
about principles

00:31:34.000 --> 00:31:36.120
I’m saying that I’m
personally aware of

00:31:36.120 --> 00:31:41.120
280 AI ethics principles documents
in the world,

00:31:41.360 --> 00:31:45.160
and of course the one that came out
of Europe is a very influential one

00:31:45.160 --> 00:31:46.400
but there are 280,

00:31:46.840 --> 00:31:49.320
and they’ve spent 3 years
on these principles

00:31:49.640 --> 00:31:51.760
while these systems are being deployed

00:31:52.640 --> 00:31:54.400
and while they are already at work.

00:31:54.560 --> 00:31:56.440
So you may or may not know that

00:31:56.440 --> 00:32:00.920
there are some very high risk cases
in the US right now.

00:32:01.840 --> 00:32:03.920
For example, the US parole system

00:32:05.000 --> 00:32:08.520
is actually managed by an AI
decision support systems,

00:32:08.520 --> 00:32:12.920
like 30% of US parole systems is
already done by machines.

00:32:14.280 --> 00:32:15.960
My worry going forward

00:32:15.960 --> 00:32:19.800
that is not very much publicised
in the media yet

00:32:20.080 --> 00:32:20.800
is that

00:32:20.960 --> 00:32:23.440
especially when we are using
AI technologies

00:32:23.440 --> 00:32:25.400
on decision support systems,

00:32:25.760 --> 00:32:28.360
we can easily face a situation

00:32:28.360 --> 00:32:31.520
when people who want to
relinquish responsibility

00:32:31.880 --> 00:32:34.800
they will be pushing these decisions
onto the machines

00:32:35.240 --> 00:32:37.920
and not act as the human in the loop

00:32:38.080 --> 00:32:39.400
that they should be.

00:32:40.160 --> 00:32:44.880
So I see it as a very direct
psychological danger

00:32:45.160 --> 00:32:47.200
and if you are looking at
some of the studies

00:32:47.200 --> 00:32:49.800
that came out of the JRC recently

00:32:50.720 --> 00:32:54.160
about government uses of AI
in the public sector

00:32:54.160 --> 00:32:55.800
there are already a few cases,

00:32:56.000 --> 00:32:58.160
I think there was one
in Poland recently

00:32:58.680 --> 00:33:03.800
where with a government
welfare support system,

00:33:04.080 --> 00:33:08.480
99% of AI decisions have
not been overruled

00:33:08.640 --> 00:33:11.880
by, you know, the public servant
who was in charge of that,

00:33:12.080 --> 00:33:14.680
which may of course mean that
the system was very good,

00:33:14.680 --> 00:33:16.720
but in my humbled opinion,

00:33:16.720 --> 00:33:21.200
it actually may mean that people
are relinquishing their obligation

00:33:21.600 --> 00:33:24.480
on acting as the human
in the loop.

00:33:24.840 --> 00:33:28.560
So generally I think it’s gonna
serve as a model.

00:33:28.960 --> 00:33:30.600
I know many governments who are

00:33:30.840 --> 00:33:33.200
very keenly watching
whatever is arising

00:33:33.200 --> 00:33:34.800
out of Europe in that regard

00:33:35.360 --> 00:33:38.240
and I personally think that
the European values

00:33:38.480 --> 00:33:41.680
have to permeate, you know,
friendly governments

00:33:41.680 --> 00:33:43.560
or the thinking of friendly
governments

00:33:43.600 --> 00:33:44.600
or aligned governments

00:33:45.120 --> 00:33:45.840
in the world

00:33:45.840 --> 00:33:48.320
because in general they are heading
in the right direction,

00:33:48.520 --> 00:33:51.120
of course they are too much
regulation-minded

00:33:51.120 --> 00:33:52.400
and I would,
you know,

00:33:52.640 --> 00:33:58.200
sometimes comment on the stifling
innovation aspect of it

00:33:58.200 --> 00:34:00.520
when I’m very in my American hat

00:34:00.520 --> 00:34:03.440
but by and large they are
very much important

00:34:03.440 --> 00:34:05.040
and they are heading
in the right direction.

00:34:05.480 --> 00:34:06.280
Thank you George.

00:34:06.960 --> 00:34:11.240
You actually took us right
into the second topic

00:34:11.240 --> 00:34:12.120
of this panel

00:34:12.160 --> 00:34:14.320
because after having described

00:34:14.320 --> 00:34:16.800
all these crucial
pieces of legislation,

00:34:17.040 --> 00:34:20.120
I would like to focus on
some more overarching topics

00:34:20.720 --> 00:34:22.800
such as the European digital regulation

00:34:22.800 --> 00:34:25.440
from the global perspective.

00:34:27.120 --> 00:34:28.760
Maybe I would turn to Eliška

00:34:28.800 --> 00:34:29.920
and ask her

00:34:31.720 --> 00:34:35.680
about this EU influence
on global regulation

00:34:35.680 --> 00:34:38.800
because we actually know that
the EU is not exactly

00:34:39.120 --> 00:34:40.920
a digital innovation powerhouse

00:34:40.920 --> 00:34:45.320
when it is compared to
the US or China

00:34:45.400 --> 00:34:49.080
and if it can nevertheless set
some global standards

00:34:49.080 --> 00:34:50.440
on digital regulation.

00:34:50.720 --> 00:34:51.080
Hi

00:34:51.320 --> 00:34:51.800
Thank you 

00:34:52.600 --> 00:34:55.520
I would actually argue with you
a little there

00:34:55.520 --> 00:34:57.720
that if Europe does something
really well

00:34:57.800 --> 00:35:00.400
it's setting the standards
for regulating.

00:35:01.440 --> 00:35:05.040
It’s not only the European Union
legislation

00:35:05.040 --> 00:35:06.960
that even if I step back
a few years,

00:35:07.560 --> 00:35:10.280
I think it was 2017 or 2018,

00:35:10.280 --> 00:35:11.880
when NetzDG in Germany,

00:35:11.880 --> 00:35:15.080
the first "anti hate speech" wall
in the world actually was adopted.

00:35:15.800 --> 00:35:19.000
We saw the use of similar walls 
suddenly popping up

00:35:19.000 --> 00:35:20.000
around the globe

00:35:20.000 --> 00:35:21.000
for better or worse

00:35:21.040 --> 00:35:24.080
because when we regulate
content specifically

00:35:24.080 --> 00:35:26.200
that’s always very much connected to

00:35:26.480 --> 00:35:28.320
a level of democratic discourse

00:35:28.320 --> 00:35:30.920
and what is actually the level
of rule of law

00:35:30.920 --> 00:35:33.120
and how it’s being upheld
by the country in question.

00:35:33.720 --> 00:35:36.920
We saw a huge impact
that GDPR actually has

00:35:37.720 --> 00:35:41.000
which was the first kind of
flagship law of the European Union

00:35:41.000 --> 00:35:43.440
that puts users fundamental rights
at the centre

00:35:43.720 --> 00:35:47.480
and started protecting
their personal data.

00:35:48.200 --> 00:35:51.160
There is a lot to discuss about
the enforcement of this law

00:35:51.160 --> 00:35:53.840
and what to anticipate
in the future,

00:35:54.440 --> 00:35:56.880
but that was definitely 
a huge game changer

00:35:56.880 --> 00:35:58.840
that inspired many legislations

00:35:59.400 --> 00:36:01.080
and legislators also
around the world.

00:36:01.600 --> 00:36:04.880
We as an organisation operate
also in Latin America,

00:36:05.320 --> 00:36:08.640
we saw responses and similar laws
being proposed

00:36:09.440 --> 00:36:12.360
in Latin America
in several different countries

00:36:12.800 --> 00:36:14.680
and also other parts
of the world

00:36:15.080 --> 00:36:17.120
and when we discuss
the United States

00:36:17.120 --> 00:36:18.840
and what kind of impact potentially

00:36:18.840 --> 00:36:20.880
the upcoming DSA and DMA
can have

00:36:22.040 --> 00:36:24.040
in terms of intermedia reliability,

00:36:24.360 --> 00:36:25.160
for instance,

00:36:26.120 --> 00:36:29.000
the section 230 of the
Communication Decency Act

00:36:29.080 --> 00:36:31.960
is being subjected to legal review.

00:36:32.240 --> 00:36:35.320
I’m now not referring to what
President Trump would like to do

00:36:35.320 --> 00:36:36.840
to the section 230.

00:36:37.480 --> 00:36:39.720
Of course, under this
Trump administration

00:36:39.720 --> 00:36:42.160
this section should
never be reopened

00:36:43.240 --> 00:36:45.520
but there are also voices

00:36:45.520 --> 00:36:47.320
from any minority rights
organisations

00:36:47.320 --> 00:36:48.320
and many other movements

00:36:48.400 --> 00:36:51.040
where blanket immunity

00:36:51.320 --> 00:36:53.160
and then again we can discuss

00:36:53.160 --> 00:36:55.840
whether that has ever been
the intention of section 230

00:36:55.840 --> 00:36:57.520
or whether this is the way how

00:36:57.520 --> 00:37:01.680
courts interpreted the same high power
put under the section 230.

00:37:02.680 --> 00:37:07.280
But it resolved to some extent these
blanket immunity for platforms

00:37:07.280 --> 00:37:11.000
though within of illegal content
and illegal activities online

00:37:11.240 --> 00:37:14.600
often cannot actually seek
satisfactory remedy or protection

00:37:14.600 --> 00:37:16.880
that they would deserve in
that particular context.

00:37:17.360 --> 00:37:19.880
There are several proposals now
coming up in the States

00:37:19.880 --> 00:37:21.280
for instance, PACT Act

00:37:21.600 --> 00:37:24.560
that we do follow quite closely

00:37:24.560 --> 00:37:27.800
and our Washington team has followed
that proposal quite closely

00:37:29.440 --> 00:37:33.560
and even had a chance to actually
submit formal comments on PACT Act

00:37:33.920 --> 00:37:35.600
during this consultation period

00:37:35.920 --> 00:37:38.000
and it was quite interesting
for me to see

00:37:38.000 --> 00:37:42.840
how many measures were actually just
taken from the European context,

00:37:43.000 --> 00:37:47.400
from 24 hours time frame
for removal of the content

00:37:47.520 --> 00:37:50.600
to ideas around notice
and actions procedures

00:37:50.600 --> 00:37:55.240
and how there are like this very niche
content governance legal aspects

00:37:55.240 --> 00:37:57.440
that European laws usually do

00:37:57.720 --> 00:37:58.760
or intend to keep.

00:37:59.760 --> 00:38:01.720
So the impact is enormous.

00:38:02.440 --> 00:38:03.600
There is of course India

00:38:03.600 --> 00:38:07.360
and their guidelines on
intermedia reliability

00:38:08.480 --> 00:38:11.840
and interestingly, we often see
these measures

00:38:11.840 --> 00:38:13.320
that we criticise in Europe,

00:38:13.320 --> 00:38:14.840
also spreading globally,

00:38:15.080 --> 00:38:18.640
so the legal naturally again
contributes to more oppression

00:38:18.640 --> 00:38:22.320
and, you know, unjustifiable
restrictions on fundamental rights

00:38:22.320 --> 00:38:24.080
and not only freedom of expression

00:38:24.880 --> 00:38:26.880
because especially when it comes to

00:38:27.320 --> 00:38:29.880
something we call potentially
harmful content

00:38:30.200 --> 00:38:33.120
which by the way should
just be outside

00:38:33.120 --> 00:38:35.160
of the scope of future
DSA framework,

00:38:35.160 --> 00:38:37.560
and we can discuss that later on,

00:38:37.560 --> 00:38:39.520
and in the European understanding,

00:38:39.520 --> 00:38:41.760
this information actually folds
into that scope,

00:38:41.960 --> 00:38:44.600
but there are other ways
how DSA can tackle that

00:38:45.160 --> 00:38:49.720
whether through proper meaningful
transparency safeguard accountability

00:38:49.720 --> 00:38:51.960
and eventually interoperability

00:38:51.960 --> 00:38:53.640
which has already been mentioned here,

00:38:53.640 --> 00:38:56.760
even though I think it also needs
to be underlined

00:38:56.760 --> 00:38:58.720
that we are very much
at the beginning,

00:38:59.200 --> 00:39:00.680
so what was published now

00:39:00.680 --> 00:39:02.760
in terms of the European
Democracy Action Plan,

00:39:02.760 --> 00:39:05.560
that was just the communication
that outlines

00:39:05.880 --> 00:39:08.320
what EDAP is actually meant
to be about,

00:39:08.320 --> 00:39:10.520
but we haven't seen yet
any concrete measures,

00:39:10.520 --> 00:39:12.920
that all will unfold
throughout the years

00:39:13.320 --> 00:39:14.920
and DSA and DMA

00:39:14.920 --> 00:39:17.680
even when the drafts will
come out on the 15th,

00:39:17.680 --> 00:39:21.320
it will take us another 2-3-4 years
or even more

00:39:22.200 --> 00:39:24.760
depending on how optimistic
or how pessimistic you are.

00:39:24.760 --> 00:39:27.440
We all remember how long
we negotiated GDPR.

00:39:28.200 --> 00:39:30.080
So everything we're saying right now

00:39:30.080 --> 00:39:32.120
it’s still at the level
of speculation

00:39:32.360 --> 00:39:33.880
and we can only hope for the best,

00:39:34.800 --> 00:39:37.640
but there are other measures
the EU can actually tackle

00:39:37.640 --> 00:39:40.160
the spread of disinformation and
the machine learning systems

00:39:40.160 --> 00:39:42.840
and amplification of potentially
harmful content,

00:39:43.000 --> 00:39:45.280
so not only how the content
is being moderated,

00:39:45.640 --> 00:39:47.280
but how it’s being distributed,

00:39:47.800 --> 00:39:50.840
that’s the key departing point

00:39:50.840 --> 00:39:53.400
that we should all see
and recognise

00:39:53.400 --> 00:39:55.800
and that’s the way how we can
then actually arrive

00:39:55.800 --> 00:39:59.280
to some sustainable solutions
to disinformation

00:39:59.280 --> 00:40:01.240
or even online hate speech

00:40:02.480 --> 00:40:05.880
and that again bring us to the
whole issue of microtargeting

00:40:05.880 --> 00:40:07.600
that has already been mentioned

00:40:07.600 --> 00:40:09.000
or online targeting,

00:40:09.400 --> 00:40:10.600
behavioural ads,

00:40:11.160 --> 00:40:12.160
ads tech,

00:40:12.720 --> 00:40:13.800
there are so many terms

00:40:13.800 --> 00:40:15.160
and they often conflate

00:40:15.160 --> 00:40:17.440
or are being conflated
in this debate. 

00:40:18.120 --> 00:40:20.560
We know that European
Parliament called

00:40:21.200 --> 00:40:24.040
for a ban on online targeting

00:40:24.040 --> 00:40:26.000
in one of its reports on DSA

00:40:27.160 --> 00:40:30.200
and all of this will need to be
unpacked and unbundled

00:40:30.720 --> 00:40:33.000
and the same applies, for instance,
to interoperability,

00:40:33.000 --> 00:40:33.920
great idea,

00:40:33.920 --> 00:40:35.440
but we need to really know

00:40:35.440 --> 00:40:38.160
what and who we are actually
making interoperable

00:40:38.440 --> 00:40:40.520
and this is something Europe
will need to figure out

00:40:41.080 --> 00:40:42.560
because the world is watching,

00:40:42.560 --> 00:40:43.440
that’s the thing,

00:40:43.440 --> 00:40:44.880
Europe does have an impact

00:40:44.880 --> 00:40:47.280
and power over regulation globally

00:40:47.600 --> 00:40:50.760
and we can cause a lot of mess
in some parts of the world

00:40:51.080 --> 00:40:52.520
if we don’t get this right,

00:40:52.760 --> 00:40:55.000
so there is a lot
at stake here.

00:40:55.640 --> 00:40:56.720
Thank you Eliška

00:40:56.720 --> 00:40:59.760
for putting our discussion
into this global perspective.

00:41:00.520 --> 00:41:03.360
Also Pavel wanted to react
on this topic

00:41:04.600 --> 00:41:08.280
of the global influence
of EU digital regulations,

00:41:08.280 --> 00:41:09.440
so please.

00:41:10.440 --> 00:41:11.480
Thank you so much Klara.


00:41:11.480 --> 00:41:14.280
I would only repeat Eliška’s words

00:41:14.280 --> 00:41:15.560
for most of the things

00:41:16.040 --> 00:41:18.960
but especially I think what
we should be really proud of

00:41:18.960 --> 00:41:21.640
is that we are really like shaping
world things, I think.

00:41:22.280 --> 00:41:24.880
Here once again the Action Plan
for Democracy

00:41:25.760 --> 00:41:27.680
it can be really an opportunity

00:41:27.840 --> 00:41:30.640
for doing that thing in the future
as well you know.

00:41:30.640 --> 00:41:32.360
GDPR, it was already mentioned,

00:41:32.360 --> 00:41:34.520
this was one of the successful cases,
you know,

00:41:34.520 --> 00:41:37.680
which is being studied or
was studied at the time

00:41:37.680 --> 00:41:38.680
in the United States,

00:41:38.680 --> 00:41:39.960
they are very much following
those trends

00:41:39.960 --> 00:41:41.800
that was mentioned previously

00:41:42.000 --> 00:41:45.120
and I think we are also
so much lucky, you know,

00:41:45.120 --> 00:41:46.400
we are now sitting,

00:41:46.720 --> 00:41:47.720
ourselves at least,

00:41:48.200 --> 00:41:49.520
in the centre of Europe,

00:41:49.520 --> 00:41:50.720
in Czechia,
in Slovakia

00:41:50.720 --> 00:41:55.320
and from our perspective,
we can be only very grateful

00:41:55.320 --> 00:41:57.320
for the EU to do all of this work,

00:41:57.320 --> 00:42:02.000
because otherwise we will not be
able at all to shape this agenda.

00:42:02.320 --> 00:42:05.080
I would just compare that
with countries,

00:42:05.080 --> 00:42:07.120
with our neighbours, you know,
from the East, you know,

00:42:07.120 --> 00:42:08.200
Ukraine, Georgia,

00:42:08.440 --> 00:42:10.600
but also the Western Balkan
countries, you know,

00:42:10.600 --> 00:42:14.600
which are only desiring to have
something like, you know,

00:42:14.840 --> 00:42:16.160
online libraries, you know,

00:42:16.160 --> 00:42:18.760
an actual law for digital
political advertising, you know.

00:42:18.760 --> 00:42:20.720
We are very critical,
you know,

00:42:20.720 --> 00:42:23.360
we are so much critical
about this that, you know,

00:42:23.600 --> 00:42:25.800
the information there are
just, you know,

00:42:26.040 --> 00:42:28.800
not really telling that

00:42:28.800 --> 00:42:30.760
the ranges are too big,
you know

00:42:30.760 --> 00:42:33.720
and the third parties are playing,
having a role

00:42:33.720 --> 00:42:36.600
and we are not monitoring them enough
and so on and so forth,

00:42:36.600 --> 00:42:39.000
but East countries are
desperately wanting

00:42:39.000 --> 00:42:40.440
at least to have this,
you know,

00:42:40.520 --> 00:42:43.000
and there Facebook,
Twitter, Google

00:42:43.000 --> 00:42:45.200
are just not delivering that
at all

00:42:45.400 --> 00:42:47.160
So what we should be doing

00:42:47.160 --> 00:42:49.440
and here I see a huge opportunity

00:42:49.440 --> 00:42:53.520
especially after January 21st,
you know, next year

00:42:53.760 --> 00:42:56.760
that there’ll be so much more
of common work

00:42:56.760 --> 00:43:01.440
from the side of the EU with 
our US counterparts on this

00:43:01.440 --> 00:43:03.720
because the big part of the gate

00:43:03.720 --> 00:43:06.520
is actually lying on
the US table,

00:43:06.520 --> 00:43:07.920
as George mentioned,

00:43:07.920 --> 00:43:09.640
so only through this interaction

00:43:09.640 --> 00:43:12.160
we can be really pushing
the platforms to develop,

00:43:12.480 --> 00:43:14.920
to deliver not only
in our own continuance,

00:43:14.920 --> 00:43:16.040
but also global,
you know.

00:43:16.040 --> 00:43:18.920
So here I see one big
opportunity as well

00:43:18.920 --> 00:43:20.920
so I just wanted to add that
to our debate.

00:43:20.920 --> 00:43:21.680
Thank you so much

00:43:22.360 --> 00:43:23.160
Thank you Pavel

00:43:24.560 --> 00:43:27.400
Quickly, I would turn back to George

00:43:28.080 --> 00:43:30.520
because I think also in this like,

00:43:30.520 --> 00:43:33.440
when we have discussions
about EU digital regulation,

00:43:34.760 --> 00:43:36.200
critics would often say that

00:43:36.400 --> 00:43:37.760
there is too much regulation

00:43:37.800 --> 00:43:39.600
and that it's making actually

00:43:40.600 --> 00:43:44.720
the EU digital companies
uncompetitive

00:43:44.720 --> 00:43:48.680
compared to their global
competitors.

00:43:50.520 --> 00:43:51.720
So I would like to ask you,

00:43:51.720 --> 00:43:53.720
do you think that AI

00:43:53.720 --> 00:43:57.080
which is respectful of human rights
and freedoms,

00:43:57.400 --> 00:43:59.560
if it can be actually competitive
in the world

00:43:59.560 --> 00:44:01.360
and maybe not just focus on the AI

00:44:01.360 --> 00:44:04.720
but general like European-based
digital technologies

00:44:05.320 --> 00:44:07.520
that are actually upholding
some standards.

00:44:10.720 --> 00:44:11.920
In the long run, yes.

00:44:12.960 --> 00:44:14.720
In the short run, not that much.

00:44:14.720 --> 00:44:17.720
I mean, let’s face our hard facts
about, you know,

00:44:18.080 --> 00:44:21.120
where European companies stand
in the global rankings

00:44:23.000 --> 00:44:24.680
and you can talk AI,

00:44:24.680 --> 00:44:25.960
you can talk digital.

00:44:26.400 --> 00:44:27.720
I think that

00:44:28.520 --> 00:44:31.400
the current European innovation
funding structure

00:44:31.400 --> 00:44:34.320
and a lot of the decision-making
processes

00:44:34.320 --> 00:44:35.320
and the

00:44:36.120 --> 00:44:38.040
Some of focuses are not
in the right place.

00:44:38.040 --> 00:44:42.320
I mean, that’s ages old debate,

00:44:42.320 --> 00:44:43.920
or I would say statement

00:44:43.920 --> 00:44:47.120
that Europe has double
innovation deficit

00:44:47.120 --> 00:44:49.080
and somehow the money

00:44:49.520 --> 00:44:52.720
that is poured into
technology and AI

00:44:52.720 --> 00:44:53.960
is not really visible,

00:44:54.640 --> 00:44:56.560
it doesn’t commercialise well,

00:44:56.560 --> 00:45:00.040
it doesn't take Europe and
European companies to the ranks.

00:45:00.600 --> 00:45:01.880
So if you wanna be cynical,

00:45:02.840 --> 00:45:04.600
you could say that,
you know,

00:45:04.960 --> 00:45:06.800
the same old adage of

00:45:07.480 --> 00:45:10.400
"If all you have is a hammer,
everything looks like a nail."

00:45:11.200 --> 00:45:14.000
If you don't have your own
technological advantage,

00:45:14.040 --> 00:45:16.080
then you regulate other people

00:45:16.720 --> 00:45:18.080
or foreign actors.

00:45:18.720 --> 00:45:19.240
And

00:45:19.880 --> 00:45:21.360
It has certain advantages

00:45:21.360 --> 00:45:23.280
because I still very much believe

00:45:23.280 --> 00:45:25.280
that Europe’s heart is
in the right place

00:45:25.920 --> 00:45:27.960
But I would definitely

00:45:28.320 --> 00:45:31.120
push the European institutions
and governments

00:45:31.120 --> 00:45:32.120
in the direction

00:45:32.320 --> 00:45:37.080
that is actually crafting
the European model of AI

00:45:38.600 --> 00:45:41.240
and there is a lot of talk,
you know, lately

00:45:41.240 --> 00:45:44.080
about sovereign IT
in the new Commission

00:45:44.080 --> 00:45:46.480
and I think it's also
very much important

00:45:46.480 --> 00:45:49.360
because Europe is between
a rock and a hard place

00:45:49.360 --> 00:45:51.960
meaning the US and China

00:45:51.960 --> 00:45:53.520
as data monopolies

00:45:54.160 --> 00:45:55.320
and they have monopolies,

00:45:55.320 --> 00:45:56.840
so sovereignty is crucial

00:45:56.840 --> 00:45:58.280
but I think that

00:45:59.440 --> 00:46:01.520
that definitely there

00:46:01.960 --> 00:46:04.360
I think there should be
a lot of regulation,

00:46:04.680 --> 00:46:06.720
it can very much become

00:46:06.880 --> 00:46:09.160
a purely legal battle brand

00:46:09.160 --> 00:46:11.760
when the companies
who own AI

00:46:12.280 --> 00:46:16.800
will try to label their
technologies as not AI

00:46:17.080 --> 00:46:22.040
and just, you know, it fails
to fulfil its purpose

00:46:22.040 --> 00:46:24.960
What does regulation need
to fail or fulfill its purpose?

00:46:25.240 --> 00:46:28.360
So now I’m putting on my,
you know, American hat,

00:46:28.640 --> 00:46:33.120
I think that Europe has to
understand these laws of AI

00:46:33.120 --> 00:46:34.200
like, you know, 

00:46:34.200 --> 00:46:37.640
how much it needs
concentration of resources,

00:46:39.040 --> 00:46:42.360
how much it needs, you know,
national laboratories,

00:46:42.360 --> 00:46:45.440
how much it needs investment
into new kinds of research

00:46:45.440 --> 00:46:49.200
that is maybe going beyond
the boundaries

00:46:49.480 --> 00:46:53.200
of where AI is at right now
in this machine learning age

00:46:53.200 --> 00:46:54.080
where we are.

00:46:54.760 --> 00:46:56.640
So there are a lot of factors

00:46:56.640 --> 00:47:00.960
that I don't think permeated the minds
of European legislators yet

00:47:01.680 --> 00:47:02.800
and instead of doing,

00:47:02.800 --> 00:47:03.800
they are regulating

00:47:03.840 --> 00:47:06.400
and they should be doing
a lot more of the doing

00:47:07.240 --> 00:47:09.960
to be actually competitive.

00:47:10.320 --> 00:47:12.000
The second thing that
I have observed

00:47:12.000 --> 00:47:13.680
in my, you know, double hats

00:47:13.760 --> 00:47:14.520
is that

00:47:14.520 --> 00:47:18.760
whenever European institutions are
coming to visit Silicon Valley

00:47:18.840 --> 00:47:23.120
they are in this double
psychological state of like,

00:47:23.120 --> 00:47:25.840
on the one hand, they are admiring
a lot of the things

00:47:25.840 --> 00:47:27.040
that are there

00:47:27.040 --> 00:47:28.880
and the innovating thinking there,

00:47:29.320 --> 00:47:31.920
but on the other hand,
they are kind on enemy ground.

00:47:32.880 --> 00:47:35.720
I think there is a lot of

00:47:36.400 --> 00:47:37.560
a lot of obsession

00:47:37.800 --> 00:47:40.240
with these big technology
companies

00:47:40.240 --> 00:47:42.160
and the purpose of regulation

00:47:42.160 --> 00:47:44.440
or the real purpose behind

00:47:44.440 --> 00:47:46.960
is, you know,
how to regulate this companies

00:47:46.960 --> 00:47:48.680
and I'm not trying to defend them.

00:47:49.280 --> 00:47:53.160
What I'm trying to be is realistic

00:47:53.600 --> 00:47:55.520
that these are companies
who own AI,

00:47:56.320 --> 00:47:59.240
they have to be,
you know, at the table,

00:47:59.800 --> 00:48:02.800
they have to be used
for the right purpose

00:48:02.800 --> 00:48:04.400
of influencing legislation,

00:48:04.400 --> 00:48:06.000
and I’m not meaning lobbying here

00:48:06.600 --> 00:48:08.840
but coming up with
technological solutions

00:48:08.840 --> 00:48:09.840
to the problems

00:48:10.000 --> 00:48:11.000
that the Commission

00:48:11.280 --> 00:48:13.520
or the European institutions
are stating. 

00:48:13.760 --> 00:48:15.680
Let me give you one example of that.

00:48:16.040 --> 00:48:17.960
A few months ago,
I was part of a

00:48:19.640 --> 00:48:23.000
direct conversation between
one the big social media companies

00:48:23.000 --> 00:48:24.520
and the Commission
and the Parliament

00:48:25.480 --> 00:48:29.440
and the biggest learning
for me was that

00:48:29.960 --> 00:48:32.880
these companies who are doing
AI research

00:48:33.000 --> 00:48:35.480
that is not being publicly known,

00:48:36.920 --> 00:48:39.720
simply, the technological approach
does not really exist

00:48:39.720 --> 00:48:41.360
inside the European institutions

00:48:41.880 --> 00:48:43.720
and these companies are
the only ones

00:48:44.200 --> 00:48:45.880
who can actually supply

00:48:46.760 --> 00:48:49.440
the solution to the problem
they have caused

00:48:50.120 --> 00:48:52.840
because that kind of thinking
doesn’t exist,

00:48:52.840 --> 00:48:55.360
that kind of deep level
understanding doesn’t exist.

00:48:55.880 --> 00:48:58.960
So all in all what I wanted
to say is that

00:48:59.360 --> 00:49:01.360
I really believe in that
these big companies

00:49:01.360 --> 00:49:07.320
should be made responsible

00:49:07.600 --> 00:49:10.080
for creating public interest
technology,

00:49:10.360 --> 00:49:13.080
so basically because a lot of 
the problems are

00:49:13.160 --> 00:49:14.760
very much business model-based,

00:49:15.320 --> 00:49:18.600
the European institutions should urge
a different way of thinking

00:49:18.600 --> 00:49:20.560
and put legislation
in this direction,

00:49:20.560 --> 00:49:22.080
or regulation in the direction

00:49:22.320 --> 00:49:26.720
of emphasising the need of creating
public interest technology's texts

00:49:26.960 --> 00:49:28.800
and only these companies
can develop

00:49:28.800 --> 00:49:30.960
these, you know, public interest
technology’s texts

00:49:31.280 --> 00:49:32.200
to make it viable.

00:49:32.280 --> 00:49:35.800
So I see a lot more involvement
and engagement possible

00:49:35.800 --> 00:49:37.600
is that of like,
you know,

00:49:38.480 --> 00:49:39.480
what currently is,

00:49:39.480 --> 00:49:41.320
which is frankly
trench warfare.

00:49:41.800 --> 00:49:42.360
Yeah

00:49:42.800 --> 00:49:43.640
Thank you George

00:49:44.120 --> 00:49:46.320
I think it also quite nicely
touched upon

00:49:46.320 --> 00:49:48.440
what Pavel said about self-regulation

00:49:48.440 --> 00:49:53.040
and if it is an actually viable
approach at all 

00:49:54.560 --> 00:49:57.040
Now we are quite approaching
the end,

00:49:57.040 --> 00:50:00.000
but I still would like to ask
one question to Marcel.

00:50:00.720 --> 00:50:04.080
Also, if you would like to
send us a question

00:50:04.080 --> 00:50:05.520
now would be the time to do that.

00:50:06.200 --> 00:50:06.760
Anyways,

00:50:07.160 --> 00:50:10.840
Marcel, you being the representative
of the European institutions here,

00:50:12.440 --> 00:50:13.840
do you think

00:50:13.880 --> 00:50:16.600
that the European Union should strive

00:50:16.800 --> 00:50:20.680
for some more like digital
sovereignty in the future?

00:50:20.960 --> 00:50:22.480
As it has been the trend

00:50:22.480 --> 00:50:24.840
or at least, it has been
the buzzword

00:50:24.840 --> 00:50:26.880
of the past couple of years

00:50:27.320 --> 00:50:30.160
or if it should try to cooperate

00:50:30.480 --> 00:50:32.040
with some other global actors

00:50:32.120 --> 00:50:33.400
maybe including the US,

00:50:33.400 --> 00:50:38.080
given that now there is going to be
maybe a more favorable situation

00:50:38.080 --> 00:50:42.480
for actual transatlantic relations?

00:50:43.400 --> 00:50:44.280
The floor is yours.

00:50:45.560 --> 00:50:46.280
Yeah, thanks

00:50:46.960 --> 00:50:47.880
If you allow me,

00:50:48.080 --> 00:50:49.680
before I answer that question,

00:50:52.240 --> 00:50:55.400
because a lot has been said
in the meantime,

00:50:55.400 --> 00:50:59.440
I would also like to share
a couple thoughts from my side

00:50:59.520 --> 00:51:03.080
on disinformation and
on artificial intelligence.

00:51:05.440 --> 00:51:08.600
When it comes to disinformation,

00:51:08.640 --> 00:51:10.960
I just wanted to stress that

00:51:11.520 --> 00:51:13.360
that’s basically not a new issue, right?

00:51:14.160 --> 00:51:17.640
Disinformation has been with us
since ever

00:51:17.960 --> 00:51:18.400
but

00:51:19.480 --> 00:51:22.800
the distribution channels
have changed

00:51:22.800 --> 00:51:25.200
and their effectivity
has changed

00:51:25.680 --> 00:51:26.280
And

00:51:27.280 --> 00:51:27.920
it's

00:51:28.600 --> 00:51:29.880
it’s also

00:51:30.120 --> 00:51:32.720
I think it needs to be stressed that

00:51:34.520 --> 00:51:38.680
that the ultimate resolution to
the problem of disinformation

00:51:39.040 --> 00:51:42.360
is improving media literacy,

00:51:42.680 --> 00:51:47.080
it’s to teach critical thinking

00:51:47.080 --> 00:51:48.160
but the problem is that

00:51:48.360 --> 00:51:50.480
of course this is a long term process

00:51:50.720 --> 00:51:54.200
and in the meantime, we need to
also take some steps

00:51:54.200 --> 00:51:58.520
in order not to let
our own society

00:51:59.120 --> 00:52:00.400
be destroyed.

00:52:01.560 --> 00:52:05.720
And one of the biggest issues
with disinformation is

00:52:06.760 --> 00:52:09.520
these information bubbles

00:52:09.880 --> 00:52:11.760
which basically only people

00:52:11.760 --> 00:52:14.000
who work closely
with the technology,

00:52:14.080 --> 00:52:16.000
either on the technology level

00:52:16.000 --> 00:52:18.200
or on legislation level

00:52:18.880 --> 00:52:20.840
realised that they actually exist

00:52:20.840 --> 00:52:24.000
and what harm it does
to the society

00:52:24.360 --> 00:52:25.360
and in that sense

00:52:29.160 --> 00:52:32.120
we need to have more
algorithm transparency.

00:52:32.360 --> 00:52:33.880
With all these large platforms

00:52:33.960 --> 00:52:38.000
where they are gatekeepers
to information

00:52:38.400 --> 00:52:41.480
and in your social network feed

00:52:41.600 --> 00:52:42.440
you have

00:52:43.480 --> 00:52:44.960
you have a feed of posts

00:52:44.960 --> 00:52:46.920
and you don’t even know why

00:52:47.560 --> 00:52:49.720
is that you are reading exactly this

00:52:50.920 --> 00:52:56.800
then it’s very difficult to fight
disinformation bubbles

00:52:56.840 --> 00:53:00.400
and the rabbit hole is just, you know,
deeper and deeper as you go,

00:53:00.680 --> 00:53:03.680
so we definitely need more 
algorithm transparency

00:53:04.400 --> 00:53:07.560
and we need to have the 
possibility to opt out

00:53:07.600 --> 00:53:09.200
from this prioritisation

00:53:09.880 --> 00:53:12.360
of articles or posts,

00:53:12.560 --> 00:53:13.000
and

00:53:13.000 --> 00:53:16.240
or users even should be able to

00:53:16.600 --> 00:53:18.360
supply with their own way

00:53:18.360 --> 00:53:21.480
how they want to prioritise
the posts on their feed

00:53:22.000 --> 00:53:24.400
and the second connected
with that is

00:53:24.720 --> 00:53:25.160
you know,

00:53:25.440 --> 00:53:27.640
we cannot let just these platforms

00:53:27.920 --> 00:53:30.240
and all the advertising ecosystem

00:53:30.240 --> 00:53:33.320
to be incentivised by
destroying our society

00:53:33.560 --> 00:53:34.320
which means

00:53:35.520 --> 00:53:37.200
that we really need to target

00:53:38.040 --> 00:53:40.480
microtargeting and
behavioral advertising

00:53:40.480 --> 00:53:42.680
which I believe should be banned.

00:53:43.280 --> 00:53:45.400
I also believe that

00:53:45.600 --> 00:53:48.800
it is already very problematic

00:53:48.800 --> 00:53:51.680
from the point of view of GDPR

00:53:52.440 --> 00:53:55.120
and maybe we’ll see

00:53:55.760 --> 00:53:58.720
something in the coming months
on that side

00:53:59.080 --> 00:54:02.640
because as much as I’m aware of

00:54:02.960 --> 00:54:04.160
there is a

00:54:05.280 --> 00:54:07.240
there’s a strategic litigation,
you know,

00:54:07.240 --> 00:54:11.720
that basically puts this
behavioral advertising

00:54:12.080 --> 00:54:14.280
against, you know,
the purpose limitation

00:54:14.280 --> 00:54:15.560
that is in GDPR

00:54:15.760 --> 00:54:17.440
which seems to be really problematic.

00:54:17.600 --> 00:54:18.120
Now

00:54:19.960 --> 00:54:21.440
with AI,

00:54:22.080 --> 00:54:23.680
I just wanted to say

00:54:24.280 --> 00:54:27.320
because there was one information,

00:54:27.320 --> 00:54:28.600
one piece of information

00:54:28.600 --> 00:54:30.720
if I really receive it correctly
from George

00:54:31.800 --> 00:54:38.000
it was that 30% of paroles are
decided by AI in the US,

00:54:38.000 --> 00:54:39.080
if that is correct,

00:54:39.080 --> 00:54:42.080
that, you know, looks
really horrible to me

00:54:42.320 --> 00:54:42.960
because

00:54:44.560 --> 00:54:45.680
well, as a matter of fact

00:54:46.080 --> 00:54:47.480
I was an opinion reporter

00:54:48.280 --> 00:54:50.560
for a report on AI used by

00:54:50.560 --> 00:54:52.080
the police and judicial authorities

00:54:52.080 --> 00:54:53.080
and criminal matters

00:54:53.320 --> 00:54:54.440
in the European Parliament

00:54:54.720 --> 00:54:58.040
and, you know, one of the issues
that we have been having

00:54:58.280 --> 00:55:00.840
So this is a topic that is very, 
you know, close to my heart

00:55:00.920 --> 00:55:02.640
but one of the issues that
we’ve been having

00:55:03.920 --> 00:55:04.920
is that

00:55:05.000 --> 00:55:06.960
if we let AI to decide

00:55:07.200 --> 00:55:10.040
then we are not building
human centric technologies,

00:55:10.160 --> 00:55:13.520
we’re not building a technology
that helps us make decisions

00:55:13.840 --> 00:55:18.280
but make decisions on their own
on our behalf

00:55:18.600 --> 00:55:20.640
and not necessarily
the best decision

00:55:20.640 --> 00:55:24.680
and it also very much,
you know,

00:55:25.160 --> 00:55:26.160
leads to

00:55:27.840 --> 00:55:30.200
deepening biases in the society.

00:55:30.200 --> 00:55:32.640
Imagine that you have an AI system

00:55:32.680 --> 00:55:36.960
that decides where the police
should deploy more forces

00:55:36.960 --> 00:55:41.920
because there is maybe more needed

00:55:41.920 --> 00:55:43.320
according to the statistics

00:55:43.720 --> 00:55:46.920
but then of course you are
going to send them

00:55:46.920 --> 00:55:47.920
in the

00:55:49.680 --> 00:55:51.520
in areas where

00:55:51.760 --> 00:55:55.320
you will subsequently discover
maybe more crime

00:55:55.320 --> 00:55:58.200
because you would have had
more police officers there

00:55:58.400 --> 00:56:00.520
and then you will send even
more police officers

00:56:00.600 --> 00:56:01.600
according to that decision,

00:56:01.680 --> 00:56:04.120
so this is the problem that we face

00:56:04.160 --> 00:56:06.280
and I believe that we need
a risk-based approach

00:56:06.440 --> 00:56:07.440
as the Commission proposes

00:56:07.520 --> 00:56:10.080
but we don’t need just two levels,

00:56:10.160 --> 00:56:11.960
like a low risk and a high risk

00:56:12.040 --> 00:56:15.240
but we need a whole range
of different risks

00:56:15.280 --> 00:56:18.240
and some technologies should
also be banned

00:56:18.800 --> 00:56:20.200
for being like super high risk

00:56:20.280 --> 00:56:22.400
like facial recognition
in public spaces

00:56:22.680 --> 00:56:24.080
because that’s very risky.

00:56:24.920 --> 00:56:27.840
Now back to your question
on digital sovereignty.

00:56:28.120 --> 00:56:30.400
Well, I don't think that these
two options that you presented

00:56:30.400 --> 00:56:32.680
are necessarily

00:56:33.680 --> 00:56:34.560
completely

00:56:35.720 --> 00:56:36.240
you know,

00:56:38.000 --> 00:56:41.040
something that cannot, you know,
work together

00:56:41.320 --> 00:56:41.920
because

00:56:42.640 --> 00:56:44.120
because I believe that

00:56:44.120 --> 00:56:48.120
yeah, we should on one hand
become more sovereign

00:56:48.480 --> 00:56:50.440
in the digital area. 

00:56:51.440 --> 00:56:58.640
We need to take back control
of technology on one hand,

00:56:58.920 --> 00:57:04.480
but also I believe that we need
to be more cooperative

00:57:04.640 --> 00:57:08.960
especially with the US

00:57:10.000 --> 00:57:14.400
and we should also set clear
rules on technology

00:57:14.400 --> 00:57:15.600
that, you know, cannot

00:57:16.120 --> 00:57:18.960
clear lines that cannot be crossed

00:57:18.960 --> 00:57:22.960
and by that we want also to
set the standard globally

00:57:22.960 --> 00:57:24.840
and here I agree with Eliška,

00:57:25.280 --> 00:57:28.080
that Europe has been doing
a really good job in this.

00:57:28.640 --> 00:57:29.840
If you look in the past,

00:57:30.640 --> 00:57:31.600
I don't know how about you

00:57:31.600 --> 00:57:33.040
but my feeling is that

00:57:33.040 --> 00:57:34.920
if it’s not clearly a regulation

00:57:34.920 --> 00:57:38.280
that goes clearly
in favour of business,

00:57:38.600 --> 00:57:41.080
it’s usually Europe that takes
the first step forward

00:57:41.080 --> 00:57:45.240
and the GDPR is the best example
probably we have at hand

00:57:45.240 --> 00:57:46.160
at this moment

00:57:46.560 --> 00:57:50.840
and I believe that we will see,
you know, this GDPR principles

00:57:50.880 --> 00:57:55.920
being followed by the rest
of the world very soon.

00:57:56.720 --> 00:57:57.720
Thank you Marcel.

00:57:58.400 --> 00:58:01.480
Unfortunately, our one hour is up.

00:58:02.200 --> 00:58:04.640
I can see that this was
a very packed panel,

00:58:04.640 --> 00:58:06.600
we actually touched upon
so many issues

00:58:06.600 --> 00:58:09.280
that I think would deserve
like a discussion on their own,

00:58:09.880 --> 00:58:12.560
so unfortunately, we didn't
unpack everything

00:58:12.560 --> 00:58:14.120
 that I would imagine we could

00:58:15.240 --> 00:58:16.000
Just at the end,

00:58:16.000 --> 00:58:17.480
I would like to share with you

00:58:18.400 --> 00:58:20.600
these results of the panel

00:58:20.600 --> 00:58:24.600
that we made at the beginning.

00:58:24.920 --> 00:58:27.800
It was actually before this discussion

00:58:27.800 --> 00:58:29.280
so we can’t really measure like

00:58:29.280 --> 00:58:31.560
if these stances have changed
since then.

00:58:31.880 --> 00:58:32.560
Nevertheless,

00:58:33.880 --> 00:58:36.560
to the question if the online platforms

00:58:36.960 --> 00:58:38.920
should be like the only ones

00:58:38.920 --> 00:58:42.600
or if we can trust
the online platforms

00:58:43.040 --> 00:58:44.640
to regulate themselves.

00:58:45.680 --> 00:58:46.520
Most people,

00:58:46.520 --> 00:58:47.840
like the vast majority of people,

00:58:47.840 --> 00:58:48.920
answered “No”,

00:58:49.000 --> 00:58:51.720
so that is quite an important call

00:58:51.720 --> 00:58:53.840
for some digital regulation

00:58:54.400 --> 00:58:57.120
from the part of the EU or States.

00:58:58.000 --> 00:59:00.520
And regarding our second question

00:59:01.160 --> 00:59:03.040
I'm sorry I have it on
my small phone here

00:59:04.440 --> 00:59:06.560
Most people said

00:59:07.040 --> 00:59:11.520
that the digital regulation
should primarily make sure

00:59:11.520 --> 00:59:13.440
that content policing is transparent,

00:59:13.440 --> 00:59:15.600
available for public scrutiny

00:59:15.760 --> 00:59:17.000
and possible to appeal,

00:59:17.160 --> 00:59:19.200
which I think it’s quite in line with

00:59:19.440 --> 00:59:21.000
what you have been saying all along.

00:59:22.400 --> 00:59:28.440
Also, our viewers have said that

00:59:29.920 --> 00:59:35.320
that it should only target
strictly illegal content

00:59:35.400 --> 00:59:37.400
on very large platforms,

00:59:39.000 --> 00:59:43.520
but also some say that it should
actually be on all platforms

00:59:43.520 --> 00:59:45.080
but only the illegal content

00:59:45.080 --> 00:59:47.800
which, of course, is not
disinformation, for instance.

00:59:48.600 --> 00:59:51.280
With this I have to thank you very much
for your participation

00:59:51.280 --> 00:59:52.800
and I would like to thank

00:59:52.920 --> 00:59:53.760
George Tilesch,

00:59:53.880 --> 00:59:54.880
Eliška Pirková,

00:59:55.200 --> 00:59:56.200
Marcel Kolaja,

00:59:56.240 --> 00:59:57.120
Pavel Havlíček

00:59:57.360 --> 00:59:59.960
I’m very much looking forward to
speaking with you again

01:00:00.400 --> 01:00:00.960
and

01:00:01.640 --> 01:00:03.520
maybe we will have a longer
discussion that time.

01:00:03.920 --> 01:00:05.560
Goodbye and have a nice day

01:00:05.840 --> 01:00:07.440
Regards from Brussels, bye

01:00:07.840 --> 01:00:08.840
Bye-bye, take care.

01:00:09.240 --> 01:00:10.400
Thank you very much, bye-bye 
