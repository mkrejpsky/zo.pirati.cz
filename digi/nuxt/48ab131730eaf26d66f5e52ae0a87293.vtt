WEBVTT

00:00:05.400 --> 00:00:07.240
Good afternoon,
ladies and gentlemen

00:00:07.320 --> 00:00:10.360
I hope you enjoyed 
the previous discussion

00:00:10.360 --> 00:00:11.920
about the education

00:00:12.320 --> 00:00:15.320
Now we would like to
switch the language 

00:00:15.320 --> 00:00:18.000
So the upcoming session is in English

00:00:18.440 --> 00:00:20.520
And we will also switch the topic 

00:00:20.880 --> 00:00:22.640
Meanwhile I would like to recommend you

00:00:22.640 --> 00:00:26.600
to check the Youtube channel 
where you can find the new videos 

00:00:26.920 --> 00:00:31.680
The new video contribution
from MEP Marcel Kolaja

00:00:32.040 --> 00:00:33.920
And from two great analysts

00:00:34.320 --> 00:00:36.920
Eliška Pírková and Pavel Havliček

00:00:37.480 --> 00:00:40.800
But right now I am glad
that I have the opportunity

00:00:40.800 --> 00:00:45.160
 to make a live interview

00:00:45.160 --> 00:00:47.200
with George Tilesch

00:00:47.800 --> 00:00:52.720
Senior Global Innovation
and Artificial Intelligence expert

00:00:53.160 --> 00:00:56.640
who is acting as a conduit
and trusted advisor

00:00:56.960 --> 00:01:00.040
between the US and the European ecosystems

00:01:00.520 --> 00:01:03.560
He is focusing on AI strategies,

00:01:03.560 --> 00:01:06.080
ethics, impacts and many more

00:01:06.320 --> 00:01:11.840
He is also co-author of a great book
dedicated to artificial intelligence

00:01:12.600 --> 00:01:17.560
The book 'BetweenBrains:
Taking Back our AI Future'

00:01:18.520 --> 00:01:22.040
I have to admit that I haven't read the book yet

00:01:22.720 --> 00:01:25.920
but of course I did a small research

00:01:26.440 --> 00:01:28.640
and what is my take that

00:01:28.640 --> 00:01:29.440
that

00:01:29.560 --> 00:01:30.960
according to you George,

00:01:31.320 --> 00:01:34.040
our age is about deciding

00:01:34.280 --> 00:01:38.960
on a future of the role
of artificial intelligence

00:01:39.720 --> 00:01:41.720
Citing your own words

00:01:42.040 --> 00:01:45.080
“It is a period of in-betweenness” 

00:01:46.000 --> 00:01:49.680
Could you tell us more
about this in-betweenness?

00:01:49.680 --> 00:01:52.400
Between what we are?

00:01:52.680 --> 00:01:54.880
Please George,
the floor is yours

00:01:56.040 --> 00:01:57.400
Thank you very much Aneta

00:01:57.400 --> 00:02:00.920
I’m very much honoured and privileged
to be with you today

00:02:00.920 --> 00:02:01.480
And

00:02:01.960 --> 00:02:04.680
And to sail under the Jolly Roger
flag with you all

00:02:05.360 --> 00:02:06.360
With this crew

00:02:08.560 --> 00:02:12.120
In-betweenness is basically derived
from the title of my book 

00:02:12.120 --> 00:02:14.640
that you have just showcased so well

00:02:15.560 --> 00:02:20.760
I'm thinking that this is the age when
we are between the different models

00:02:21.040 --> 00:02:21.920
First of all,

00:02:22.240 --> 00:02:25.800
because of the core of my activities
are about artificial intelligence

00:02:25.800 --> 00:02:27.960
This is the in-betweenness age

00:02:28.160 --> 00:02:30.240
between human and machine intelligence

00:02:30.800 --> 00:02:32.240
And don't get me wrong about that 

00:02:32.240 --> 00:02:35.960
I'm not gonna, you know,
play Silicon valley profit here

00:02:35.960 --> 00:02:38.560
Who are talking about
the innovate ability

00:02:38.560 --> 00:02:40.600
of machine intelligence taking over

00:02:41.120 --> 00:02:44.280
What I'm trying to call the attention
of the audience to is that

00:02:44.280 --> 00:02:47.080
it can be a very extended period of time

00:02:47.280 --> 00:02:49.040
When we have to learn how to interact

00:02:49.040 --> 00:02:51.720
and how to engage
with machine intelligence 

00:02:51.720 --> 00:02:53.280
and how they will transform us

00:02:53.280 --> 00:02:54.920
and how we can define

00:02:56.320 --> 00:02:58.600
machine intelligence or
artificial intelligence

00:02:58.600 --> 00:03:00.880
to serve the purpose of our civilisation 

00:03:01.880 --> 00:03:04.880
So we are also between different models

00:03:04.880 --> 00:03:06.280
and when you said that

00:03:07.080 --> 00:03:09.480
that this is, you know,
the time of model setting

00:03:09.480 --> 00:03:10.400
or vision setting 

00:03:10.400 --> 00:03:13.160
I would say this should be the time
of vision setting

00:03:13.480 --> 00:03:15.080
and I think we are a little late

00:03:16.440 --> 00:03:18.360
because there is a

00:03:18.880 --> 00:03:21.800
 there is a big discrepancy 
between our readiness

00:03:21.800 --> 00:03:23.800
 to societies and economies

00:03:24.320 --> 00:03:28.680
and the prevalence of
machine intelligence systems,

00:03:28.680 --> 00:03:30.440
artificial intelligence systems

00:03:30.440 --> 00:03:32.760
that already deployed
for years now 

00:03:33.640 --> 00:03:38.320
You know, in governments and in 
commercial services that we are using

00:03:38.320 --> 00:03:40.160
So it surrounds us already

00:03:40.160 --> 00:03:43.400
even without most of the people and
most of the citizen knowing about it

00:03:44.920 --> 00:03:47.000
Another aspect of the in betweenness is,
 you know,

00:03:47.000 --> 00:03:49.400
this age of experimentation


00:03:49.840 --> 00:03:51.640
and these deployed

00:03:51.640 --> 00:03:53.920
the reality of these deployed systems 

00:03:53.920 --> 00:03:56.040
So many organisations in the world

00:03:56.040 --> 00:03:58.200
including European governments
and companies

00:03:58.600 --> 00:04:00.320
are experimenting with these systems

00:04:00.320 --> 00:04:01.840
and learning the rules of it

00:04:01.840 --> 00:04:04.400
and, you know, 
they have many failed projects

00:04:04.400 --> 00:04:06.720
they have a few good projects
in that regard

00:04:07.320 --> 00:04:09.160
But in other parts of the world,

00:04:09.160 --> 00:04:11.280
Especially in China and the US,

00:04:11.280 --> 00:04:12.480
these systems are,

00:04:12.680 --> 00:04:17.480
 you know, much more deeply ingrained
in the fabric of society

00:04:17.840 --> 00:04:21.520
And finally I think that the word of
warning here is that

00:04:22.640 --> 00:04:25.840
The last aspect that I wanted to
grasp is that we're

00:04:26.440 --> 00:04:29.480
potentially between a welfare society

00:04:29.480 --> 00:04:31.040
especially here in Europe

00:04:31.240 --> 00:04:34.280
And if we are not taking back our AI future

00:04:34.280 --> 00:04:36.520
If we are not defining the purpose well

00:04:36.920 --> 00:04:39.080
 and our new social economy models,

00:04:39.400 --> 00:04:41.200
it can easily lead to a

00:04:41.480 --> 00:04:43.320
Some kind of a dystopian future

00:04:43.880 --> 00:04:46.800
And that's a big danger because

00:04:47.120 --> 00:04:48.280
there are rules,

00:04:48.280 --> 00:04:51.480
there are laws that are ingrained
in the technology

00:04:51.960 --> 00:04:53.960
that have not been seen before

00:04:55.600 --> 00:04:57.520
And we have to get used to it

00:04:57.600 --> 00:04:58.280
So

00:04:58.840 --> 00:04:59.360
Again

00:04:59.360 --> 00:05:01.080
 I think that the last effort,

00:05:01.080 --> 00:05:03.160
the last in-betweenness 
that I wanted to grasp

00:05:03.360 --> 00:05:05.240
is in our thinking,
in our minds

00:05:06.280 --> 00:05:11.680
We have been in this age called digital
transformation for decades now

00:05:12.080 --> 00:05:17.640
and we have navigated that age with 
a certain incremental approach

00:05:18.360 --> 00:05:21.960
and I don't think our brains
are wired in a way that are

00:05:22.240 --> 00:05:24.400
getting used to the exponential

00:05:24.400 --> 00:05:26.800
and I think AI is very much
exponential

00:05:27.000 --> 00:05:29.080
It develops in leaps and bounds

00:05:29.080 --> 00:05:31.600
It gives enormous power to
the ones who own it

00:05:32.680 --> 00:05:33.960
and who manage it 

00:05:34.760 --> 00:05:36.520
So that kind of leap,

00:05:36.520 --> 00:05:39.160
mental leap from incremental
to exponential

00:05:39.160 --> 00:05:40.920
I think is very very important

00:05:40.920 --> 00:05:44.600
and should be the defining
part of our age

00:05:46.120 --> 00:05:47.280
Thank you very much George

00:05:47.760 --> 00:05:50.760
You said that our brains are,
let's say,

00:05:50.760 --> 00:05:54.880
not ready yet for the future developments

00:05:55.200 --> 00:05:57.920
But I have to say that for many people

00:05:57.920 --> 00:06:01.680
We also discussed it before
in the previous panels

00:06:02.120 --> 00:06:03.120
That, for example,

00:06:04.120 --> 00:06:07.160
machine intelligence or
artificial intelligence

00:06:07.560 --> 00:06:12.760
for many people, it is only another
technical term, let's say

00:06:12.760 --> 00:06:15.760
It's only another buzzword maybe

00:06:16.640 --> 00:06:18.160
So according to you,

00:06:18.160 --> 00:06:23.960
how the AI so much differs
in this sense?

00:06:23.960 --> 00:06:24.520
Like

00:06:24.720 --> 00:06:28.920
 What's so revolutionary on artificial intelligence?

00:06:31.880 --> 00:06:34.760
Well, I can approach this
from multiple angles 

00:06:34.760 --> 00:06:36.480
So let's start with

00:06:36.480 --> 00:06:37.520
So this is a trade,

00:06:37.520 --> 00:06:38.520
you're absolutely right in that

00:06:38.520 --> 00:06:41.080
this is a trade
that I'm encountering it everyday

00:06:41.080 --> 00:06:42.880
 from citizens,
from politicians,

00:06:43.920 --> 00:06:45.720
even from some corporate leaders 

00:06:46.000 --> 00:06:49.080
And there are two fallacies
regarding that

00:06:49.280 --> 00:06:51.800
I would say number one fallacy is that

00:06:51.800 --> 00:06:53.320
this is just another wave,

00:06:53.480 --> 00:06:55.720
this is like, you know,
big data awards

00:06:55.720 --> 00:06:58.280
or cloud computing awards before

00:06:58.480 --> 00:06:59.840
and this is all part of this

00:06:59.840 --> 00:07:02.640
incremental digital
transformation process

00:07:02.640 --> 00:07:04.120
 that I've talking to you about 

00:07:04.120 --> 00:07:06.680
I think we cannot be
more wrong than that

00:07:07.640 --> 00:07:08.800
Secondly, of course,

00:07:08.800 --> 00:07:10.640
because it’s rooted in technology

00:07:10.640 --> 00:07:12.920
most people who don't feel technical

00:07:12.920 --> 00:07:16.360
or actually already feel lost
in this digital world

00:07:16.360 --> 00:07:17.880
And many citizens are like that

00:07:17.960 --> 00:07:18.520
You know

00:07:18.520 --> 00:07:21.760
 there is still a relatively low level
of digital literacy

00:07:21.760 --> 00:07:22.720
But we are still

00:07:22.720 --> 00:07:24.040
You know everybody uses Facebook 

00:07:24.040 --> 00:07:25.040
Everybody uses

00:07:27.000 --> 00:07:30.360
these AI powered products
to gather information,

00:07:30.360 --> 00:07:31.920
To make consumer choices,

00:07:31.920 --> 00:07:34.400
To choose entertainment
and so on and so forth

00:07:35.560 --> 00:07:38.000
 without understanding what it is

00:07:38.520 --> 00:07:40.280
So taking a step back

00:07:41.240 --> 00:07:43.440
and that may sound a little bit
cynical from me

00:07:43.440 --> 00:07:45.400
but its not meant to be such

00:07:45.400 --> 00:07:46.000
But

00:07:46.440 --> 00:07:47.360
 I think AI,

00:07:47.360 --> 00:07:50.280
artificial intelligence is mostly a
marketing tool right now

00:07:50.880 --> 00:07:53.880
Actually the accurate use of it
would be something like

00:07:53.880 --> 00:07:56.200
artificial intelligence technologies 

00:07:56.200 --> 00:07:57.320
because its a wide,

00:07:57.320 --> 00:08:00.560
it's a broad set of technologies

00:08:00.560 --> 00:08:01.040
including, you know,

00:08:01.040 --> 00:08:03.920
computer vision and
natural language processing,

00:08:04.360 --> 00:08:06.360
decision support systems

00:08:06.720 --> 00:08:11.280
So there are so many technologies
under this umbrella of AI 

00:08:12.280 --> 00:08:13.720
And the reason why is that

00:08:13.960 --> 00:08:15.920
It was probably about 5 years ago 

00:08:15.920 --> 00:08:18.640
when companies started
betting big on these

00:08:18.640 --> 00:08:20.640
So they had a pressure

00:08:21.040 --> 00:08:25.160
 on commercializing whatever
it’s coming out of their labs

00:08:25.160 --> 00:08:27.520
and they needed 
very handy expression 

00:08:27.840 --> 00:08:32.800
so they basically revived it
from obscurity

00:08:33.560 --> 00:08:34.760
But the truth is,

00:08:34.760 --> 00:08:36.680
The scientific truth is that

00:08:36.680 --> 00:08:37.480
First of all,

00:08:37.680 --> 00:08:41.000
not all AI researchers agree that

00:08:41.400 --> 00:08:43.600
whatever we have out there right now

00:08:43.800 --> 00:08:46.080
 can be scientifically labeled AI

00:08:46.080 --> 00:08:47.800
so there are a lot of people saying that

00:08:48.320 --> 00:08:49.160
machine learning

00:08:49.160 --> 00:08:53.520
which is the most prevalent AI technology right now

00:08:53.520 --> 00:08:55.000
 is not exactly AI,

00:08:55.000 --> 00:08:56.160
 it's not there yet

00:08:56.160 --> 00:08:58.680
It's just sophisticated data work

00:09:00.360 --> 00:09:02.600
So I'm not gonna go into
this scientific direction

00:09:02.600 --> 00:09:04.040
because what I care about

00:09:04.040 --> 00:09:06.440
and probably what most of
your audience is about

00:09:06.840 --> 00:09:08.560
is the impact of these technologies

00:09:08.560 --> 00:09:10.760
not the technological
background of it

00:09:10.760 --> 00:09:13.320
but what do they do with us as citizens

00:09:13.320 --> 00:09:16.000
And, you know, you guys as policy makers

00:09:16.000 --> 00:09:19.440
 you are probably approaching it
from this angle

00:09:20.480 --> 00:09:22.480
What creates further confusion is

00:09:22.560 --> 00:09:26.160
when you are trying to
delineate the pragmatic

00:09:26.160 --> 00:09:29.880
 and the aspirational aspects of
 defining artificial intelligence

00:09:30.280 --> 00:09:32.200
The truth of the matter is that

00:09:32.320 --> 00:09:34.840
this is a process that has been going in

00:09:34.840 --> 00:09:36.640
 for about 60 years now

00:09:38.000 --> 00:09:41.800
through periods that are called
AI winters and AI summers 

00:09:43.120 --> 00:09:45.440
And for you just to understand this terminology

00:09:45.440 --> 00:09:46.400
AI winter

00:09:46.840 --> 00:09:50.080
I called it also mildly humorously
or mildly cynically

00:09:50.560 --> 00:09:54.320
are the ages when we are having
overinflated expectations

00:09:54.760 --> 00:09:56.920
for what AI can provide

00:09:56.920 --> 00:10:00.880
and when the technology actually
doesn't live up to these expectations 

00:10:01.080 --> 00:10:03.360
Then, you know,
funding is disappearing

00:10:03.360 --> 00:10:06.880
and all the scientists or the
researchers are going back to the lab 

00:10:06.880 --> 00:10:09.840
and try to come up with something
that is more viable

00:10:10.080 --> 00:10:13.000
So what I'm trying to say is that
I think five years ago,

00:10:13.000 --> 00:10:14.800
about five years ago
everything changed

00:10:14.800 --> 00:10:15.880
The whole equation changed 

00:10:15.880 --> 00:10:16.880
The paradigm changed

00:10:17.280 --> 00:10:18.920
Corporations and governments

00:10:19.320 --> 00:10:21.160
Venture capitals
and everybody started

00:10:21.960 --> 00:10:22.840
 pouring money

00:10:23.280 --> 00:10:24.880
and throwing money against that

00:10:26.120 --> 00:10:29.240
Against all kinds of artificial
intelligence development

00:10:30.240 --> 00:10:30.760
Oh but

00:10:30.880 --> 00:10:32.200
 there's another thing
that I wanted to say 

00:10:32.200 --> 00:10:34.240
So 60 years ago,

00:10:34.240 --> 00:10:36.560
 the earliest definitions of AI

00:10:37.080 --> 00:10:41.160
Have already introduced the term 
'human equivalent'

00:10:41.160 --> 00:10:42.960
So the original intent of

00:10:42.960 --> 00:10:45.120
you know, very very 
great scientist in the US

00:10:45.120 --> 00:10:46.800
who started researching that

00:10:46.800 --> 00:10:49.360
was to create human equivalent

00:10:49.760 --> 00:10:52.160
artificial intelligence and
machine intelligence

00:10:52.760 --> 00:10:55.560
The systems that are out there right now

00:10:55.560 --> 00:10:58.520
and most of the research that is
actually happening right now

00:10:58.520 --> 00:11:01.560
is really far from that kind of
human equivalency

00:11:01.840 --> 00:11:03.520
but is already commercialised

00:11:03.520 --> 00:11:06.400
And that's I think
the most important part of it

00:11:06.400 --> 00:11:07.080
is that

00:11:07.520 --> 00:11:10.520
it doesn't live up to the
aspirational standards

00:11:10.520 --> 00:11:13.280
of creating artificial
general intelligence

00:11:13.280 --> 00:11:16.880
so you know, the super intelligence
of The Terminator movies 


00:11:18.280 --> 00:11:20.840
But it's something that is robots

00:11:20.840 --> 00:11:23.840
It's something that is absolutely
going beyond technology

00:11:24.160 --> 00:11:25.640
It has its own laws

00:11:26.360 --> 00:11:29.440
It prefers, you know,
concentration of resources

00:11:29.520 --> 00:11:30.120
It

00:11:30.880 --> 00:11:32.600
And if you're reading my book

00:11:32.600 --> 00:11:35.160
 there are like 5 or 6
major power laws

00:11:35.480 --> 00:11:36.960
that are specific to AI

00:11:37.200 --> 00:11:38.440
that have been laid out


00:11:38.920 --> 00:11:40.960
Which should define the strategies

00:11:40.960 --> 00:11:43.320
and the way of thinking how, you know,

00:11:43.320 --> 00:11:44.960
governments and corporations or

00:11:44.960 --> 00:11:46.920
leaders in general should think about

00:11:47.720 --> 00:11:49.240
devising their own strategies

00:11:49.240 --> 00:11:52.560
Because there are many many elements
that are very specific to AI

00:11:52.840 --> 00:11:54.280
that have not been seen before

00:11:54.280 --> 00:11:57.760
or have not been seen before
in such concentration of power

00:11:59.000 --> 00:12:01.360
And they should definitely heed the call


00:12:01.760 --> 00:12:03.440
One last thing that I wanted to say

00:12:03.440 --> 00:12:05.080
 and that's very Europe specific 

00:12:05.120 --> 00:12:07.120
You mentioned that I live in California 

00:12:07.120 --> 00:12:10.720
I sometimes have the honour
and the privilege of being listened to

00:12:10.720 --> 00:12:12.440
by the European institutions

00:12:13.280 --> 00:12:17.280
Especially in the future
work aspects of AI

00:12:17.720 --> 00:12:22.720
and one thing that I'm encountering
in Europe sometimes is this kind of

00:12:23.200 --> 00:12:24.880
This is just another wave

00:12:25.200 --> 00:12:25.680
Or

00:12:25.800 --> 00:12:27.440
you know, we know what we are doing,

00:12:27.440 --> 00:12:31.280
we don't have to listen
to the Americans or to the Chinese,

00:12:31.280 --> 00:12:34.800
we absolutely grasp the essence of AI

00:12:34.800 --> 00:12:35.120
Just

00:12:35.360 --> 00:12:35.880
You know

00:12:36.040 --> 00:12:37.760
give us more funding, do your commission

00:12:38.040 --> 00:12:39.720
and we will get it done

00:12:40.040 --> 00:12:42.840
So this is also a very dangerous
way of thinking

00:12:42.840 --> 00:12:45.760
I think it's a very introverted
way of thinking

00:12:46.640 --> 00:12:48.480
but you know we can talk about more

00:12:48.480 --> 00:12:51.320
 more about the geopolitical aspects
or who's winning

00:12:51.880 --> 00:12:52.600
in that regard

00:12:53.240 --> 00:12:53.720
Yeah 

00:12:53.720 --> 00:12:55.560
That's also my follow up question

00:12:55.560 --> 00:12:57.520
 because for me as a journalist

00:12:57.520 --> 00:12:59.480
 who is covering European issues

00:12:59.800 --> 00:13:02.600
I can see that many countries in Europe,

00:13:02.800 --> 00:13:04.120
but also in the whole world

00:13:04.720 --> 00:13:08.120
are declaring that they want to be the leaders

00:13:08.320 --> 00:13:10.480
 in the artificial intelligence 

00:13:10.720 --> 00:13:14.920
Even the Czech Republic wants to be the
leaders in the artificial intelligence

00:13:16.280 --> 00:13:19.800
So do you think that given this race

00:13:19.800 --> 00:13:21.280
between countries 

00:13:21.760 --> 00:13:26.240
can the artificial intelligence become

00:13:26.640 --> 00:13:32.080
 source of kind of geopolitical or
international tensions?

00:13:34.480 --> 00:13:35.480
It already is 

00:13:36.000 --> 00:13:37.960
I would say,
it very much is

00:13:39.840 --> 00:13:41.280
Where to approach it from?
 

00:13:41.600 --> 00:13:42.360
So

00:13:43.200 --> 00:13:44.200
Let's start with China 

00:13:46.080 --> 00:13:49.320
China is not the most important
or the most powerful player

00:13:49.320 --> 00:13:50.120
on that scene

00:13:50.120 --> 00:13:52.520
but it has the biggest momentum,
I would say 

00:13:52.520 --> 00:13:55.560
And it has the biggest
concentration of resources

00:13:56.760 --> 00:14:00.720
So it has the advantages of
initiative in AI 

00:14:01.240 --> 00:14:05.760
Since 2015- 2016,
China went all in on AI

00:14:05.800 --> 00:14:09.480
Basically it became the core strategy
of the Chinese government

00:14:09.840 --> 00:14:12.560
both in its international
expansion efforts

00:14:12.560 --> 00:14:18.600
and devising an AI powered
operating system

00:14:18.600 --> 00:14:20.680
for its society
let's call it that 

00:14:20.680 --> 00:14:22.360
And there are, you know,
parts of it that

00:14:22.360 --> 00:14:24.560
you probably have heard about
as a journalist

00:14:24.560 --> 00:14:26.120
like the social credits system

00:14:26.440 --> 00:14:27.760
and so on and so forth 

00:14:28.160 --> 00:14:31.720
So I think we have to fully
understand how

00:14:32.080 --> 00:14:34.480
how China is betting on AI and AI only

00:14:34.480 --> 00:14:35.320
or mostly

00:14:35.320 --> 00:14:37.960
So China has a huge
concentration of resources

00:14:37.960 --> 00:14:40.600
and zero boundaries
between state and industry

00:14:40.600 --> 00:14:42.600
I think that's my most
important statement

00:14:42.920 --> 00:14:46.040
So Chinese private tech companies

00:14:46.520 --> 00:14:49.720
are listening to central orders

00:14:50.080 --> 00:14:51.920
and they have been given the tasks

00:14:51.920 --> 00:14:54.760
 of delivering on certain aspects 

00:14:54.760 --> 00:14:58.480
or certain sets of AI technologies 

00:14:59.800 --> 00:15:02.440
So there is this concentration
of investment

00:15:02.640 --> 00:15:04.280
that I think it's very worth it

00:15:04.560 --> 00:15:08.280
and also there is something
that we would call 

00:15:08.280 --> 00:15:10.640
Or that we can call
data monopolies

00:15:11.080 --> 00:15:13.040
and data colonization

00:15:13.800 --> 00:15:16.080
 and the data monopolies is
something that

00:15:16.080 --> 00:15:21.240
is absolutely driven by
 both the US and China right now

00:15:21.480 --> 00:15:23.000
The data colonisation aspects

00:15:23.000 --> 00:15:25.480
I think is more prevalent
in the case of China

00:15:25.800 --> 00:15:26.480
which means

00:15:26.560 --> 00:15:28.040
you probably heard that,
you know

00:15:28.240 --> 00:15:31.680
Most of Africa is already being
kind of dominated

00:15:31.680 --> 00:15:33.720
by Chinese international expansion 

00:15:33.760 --> 00:15:36.800
and of course the whole
South East Asian region

00:15:37.280 --> 00:15:40.680
So that is very much central
in the strategy 

00:15:41.200 --> 00:15:42.800
And the US has the

00:15:43.440 --> 00:15:45.480
I would say,
the most potential

00:15:45.480 --> 00:15:47.840
or the most historical advantages

00:15:48.640 --> 00:15:49.640
but

00:15:50.080 --> 00:15:52.680
I'm working with a number of,
you know,

00:15:52.680 --> 00:15:55.200
a number of committees
inside the US government

00:15:55.200 --> 00:15:56.640
or with the US government

00:15:57.040 --> 00:16:00.680
and what I see is that there were
a few years that have been

00:16:01.280 --> 00:16:03.120
slower than necessary 

00:16:03.160 --> 00:16:05.520
I wouldn't call it
wasted completely 

00:16:05.520 --> 00:16:07.680
but definitely
slower than necessary 

00:16:07.680 --> 00:16:10.160
So the US was not taking the angle

00:16:10.160 --> 00:16:12.120
of what other countries have been

00:16:13.640 --> 00:16:17.080
taking of, you know, putting together
AI national strategies

00:16:17.080 --> 00:16:18.200
until quite late

00:16:18.480 --> 00:16:21.440
So I would say 50-60 countries
have already published

00:16:21.440 --> 00:16:22.600
their national strategies

00:16:22.600 --> 00:16:24.720
before the US
took its first step 

00:16:27.160 --> 00:16:29.280
What happened in the US is probably

00:16:30.200 --> 00:16:34.200
the China compete mentality
has taken over

00:16:34.520 --> 00:16:37.680
so whatever hyper-speed that
I’m experiencing

00:16:37.680 --> 00:16:39.560
inside the US federal government

00:16:39.560 --> 00:16:41.240
 for, I would say,
the last 6 months

00:16:41.520 --> 00:16:44.800
 is very much in this context of
competing with China

00:16:44.800 --> 00:16:46.680
and catching up with
some of the momentum 

00:16:47.040 --> 00:16:48.000
that they are having

00:16:48.000 --> 00:16:50.640
and building on the
traditional US advantages

00:16:51.160 --> 00:16:53.080
And the reason why that happened is
that

00:16:53.080 --> 00:16:54.520
for a long time
I think

00:16:54.520 --> 00:16:57.640
And you need to understand
the US culturally for that 

00:16:58.240 --> 00:17:00.720
So for a long time,
the US government

00:17:00.720 --> 00:17:02.560
especially the Trump administration,

00:17:02.560 --> 00:17:03.080
thought

00:17:03.440 --> 00:17:06.360
 that as long as, you know,
the dominance

00:17:06.360 --> 00:17:08.640
of the big tech companies in the US

00:17:08.640 --> 00:17:11.920
who are one and the same with
the AI leaders of the world

00:17:13.280 --> 00:17:15.840
is being insured in
the international scene

00:17:15.840 --> 00:17:17.320
then there's not much to be done

00:17:17.320 --> 00:17:19.280
 other than, you know,
the usual spendings

00:17:19.280 --> 00:17:20.680
on defence and intelligence 

00:17:20.680 --> 00:17:22.840
which is also very central
to American thinking 

00:17:23.240 --> 00:17:23.920
And that

00:17:23.920 --> 00:17:27.600
that was, you know, what held them back
for quite a long time 

00:17:27.840 --> 00:17:29.360
Now they're very much catching up

00:17:29.360 --> 00:17:31.400
I’m seeing a lot of momentum

00:17:31.400 --> 00:17:32.960
and a lot of innovating thinking 

00:17:32.960 --> 00:17:34.960
that has not been seen before 

00:17:34.960 --> 00:17:36.640
I’m just giving you one example

00:17:37.960 --> 00:17:41.760
I just heard that you had
an education panel before me

00:17:41.920 --> 00:17:44.000
even if I don't speak Czech
I'm very sorry 

00:17:45.800 --> 00:17:48.920
But the type of thinking
that the EU has

00:17:49.040 --> 00:17:50.760
Has not been,

00:17:50.760 --> 00:17:53.520
on education and massive
education programs

00:17:53.520 --> 00:17:55.880
 that, you know, should be
publicly funded 

00:17:56.160 --> 00:18:00.360
It's not being part of the US DNA
for a long long long time

00:18:01.040 --> 00:18:03.080
Actually it was like,
I think 5

00:18:04.640 --> 00:18:11.680
0.05% of the GDP that has been spent
on public education programs 

00:18:11.920 --> 00:18:13.480
and that's, 
you know,

00:18:13.480 --> 00:18:17.720
Now I'm thinking of company turnaround
in the US government

00:18:17.720 --> 00:18:20.560
and now they have realised and
they are actually doing a lot of

00:18:20.560 --> 00:18:21.800
pioneering initiatives

00:18:22.160 --> 00:18:24.520
 on how to retrain
the government workforce

00:18:24.520 --> 00:18:26.480
I'm talking tenths of millions people

00:18:26.720 --> 00:18:28.600
 to be AI compatible

00:18:28.600 --> 00:18:30.320
and having AI compatible jobs 

00:18:30.320 --> 00:18:31.440
So this is very novel

00:18:31.440 --> 00:18:32.440
and very very new 

00:18:33.040 --> 00:18:33.600
Now Europe

00:18:33.600 --> 00:18:36.160
and of course this is where I'm getting
a little bit more cautious

00:18:36.160 --> 00:18:36.720
because

00:18:37.040 --> 00:18:38.520
I'm not living here anymore but

00:18:38.560 --> 00:18:40.680
I'm keeping my eyes on that 

00:18:41.080 --> 00:18:41.600
So

00:18:42.720 --> 00:18:47.560
The EU I think is putting the emphasis
on a value centric approach 

00:18:47.560 --> 00:18:48.440
in AI,

00:18:48.440 --> 00:18:51.040
in, you know,
building trust worthy AI 

00:18:51.360 --> 00:18:53.960
and of course it does
what it usually does

00:18:53.960 --> 00:18:55.560
the regulatory approach 

00:18:55.920 --> 00:18:58.440
which has certain advantages


00:18:58.440 --> 00:19:00.800
and certain disadvantages 

00:19:01.920 --> 00:19:05.320
I believe that in the long run
it may be a long term positive

00:19:06.640 --> 00:19:07.800
Of course it needs bets 

00:19:07.800 --> 00:19:10.480
I think that The EU currently
is betting on 

00:19:10.480 --> 00:19:11.760
for one reason or another

00:19:11.760 --> 00:19:12.480
is betting on

00:19:12.480 --> 00:19:15.520
that the spread of AI technologies
will be slower

00:19:15.800 --> 00:19:18.480
 than how I expect it to be 

00:19:18.840 --> 00:19:21.440
and if they are right,

00:19:21.440 --> 00:19:24.440
then their caution is well deserved

00:19:24.640 --> 00:19:28.640
but I think in the long run the EU
can become something like a very

00:19:28.640 --> 00:19:33.000
popular destination of
attracting talent from abroad

00:19:33.280 --> 00:19:37.760
because it's focused on values and
just security and that kind of thing

00:19:38.480 --> 00:19:38.920
Go ahead

00:19:39.160 --> 00:19:39.760
Great

00:19:40.080 --> 00:19:41.360
Thank you very much George 

00:19:41.360 --> 00:19:43.160
for this great insight

00:19:43.160 --> 00:19:45.600
when it comes to artificial intelligence,

00:19:45.600 --> 00:19:47.680
Governments and the whole

00:19:48.560 --> 00:19:51.120
important and interesting topics
you mentioned

00:19:52.280 --> 00:19:53.960
Our time is over now

00:19:53.960 --> 00:19:57.880
Because in the next 2 minutes
the new session

00:19:57.880 --> 00:19:59.760
The new panel discussion is beginning 

00:20:00.640 --> 00:20:04.360
The topic is the future of
European digital policy

00:20:04.720 --> 00:20:08.160
and it's great that you will
also join this panel  as well

00:20:08.480 --> 00:20:10.400
So thank you very much 

00:20:10.400 --> 00:20:13.880
and I'm looking forward to
the next discussion 

00:20:13.880 --> 00:20:16.280
that is starting in the next 2 minutes

00:20:16.400 --> 00:20:17.800
So thank you very much


00:20:18.480 --> 00:20:19.480
Thank you very much Aneta
